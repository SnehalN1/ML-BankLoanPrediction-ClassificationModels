{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"a09b50b5\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Bank Loan Prediction Model\\n\",\n",
    "    \"## Machine Learning: Classification Models Comparison\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"4e9c22cf\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"<p>\\n\",\n",
    "    \"<img src = 'https://www.virginstartup.org/sites/startup.virgin.com/files/tips_on_getting_a_bank_loan.jpg'\\n\",\n",
    "    \"     height = \\\"400\\\"\\n\",\n",
    "    \"     width = \\\"900\\\"/>\\n\",\n",
    "    \"</p>\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"dd47e6ac\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Table of Contents: <a class=\\\"anchor\\\" id=\\\"table-of-contents\\\"></a>\\n\",\n",
    "    \"* [1. Problem Background and Motivation](#problem)\\n\",\n",
    "    \"* [2. Libraries and Custom Functions](#libcustfunc)\\n\",\n",
    "    \"* [3. Data Exploration and Data Prep](#dataexp)\\n\",\n",
    "    \"* [4. Understanding the Business Problem](#busprob)\\n\",\n",
    "    \"* [5. Machine Learning Models](#maclrn)\\n\",\n",
    "    \"* [6. Model Comparison](#modelcomp)\\n\",\n",
    "    \"* [7. Model Selection](#modelsel)\\n\",\n",
    "    \"* [8. Deployment of Model](#deploy)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"c9ba75b4\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Problem Background and Motivation <a class=\\\"anchor\\\" id=\\\"problem\\\"></a>\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"8809d184\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"<b>I. High Level Outline</b>\\n\",\n",
    "    \"\\n\",\n",
    "    \"_Target Variable: approved_<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here we are dealing with the loan details of a large regional bank in order to predict whether or not we can approve loans of new customers. We are presented with data of 689 loan applications including those loans which were not approved.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src = 'https://guardhill.com/wp-content/uploads/2021/03/5cscreditblogchart-copy.png'\\n\",\n",
    "    \"     height = \\\"325\\\"\\n\",\n",
    "    \"     width = \\\"380\\\"\\n\",\n",
    "    \"     align = \\\"right\\\"/>\\n\",\n",
    "    \"     \\n\",\n",
    "    \"Typically, banks consider the 5 Cs of Credit while assessing any potential borrower:\\n\",\n",
    "    \"- Character\\n\",\n",
    "    \"- Capacity\\n\",\n",
    "    \"- Capital\\n\",\n",
    "    \"- Collateral\\n\",\n",
    "    \"- Conditions\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here, while we may have been given a limited number of data points to work with, I expect the following things shall need to be looked at while predicting task completion (including existing parameters):\\n\",\n",
    "    \"- Debt level\\n\",\n",
    "    \"- Existing bank customer or not (recorded conduct and history with the bank) \\n\",\n",
    "    \"- Industry that the applicant belongs to (is employed in/ has own business)\\n\",\n",
    "    \"- Number of years of employment\\n\",\n",
    "    \"- Prior defaults\\n\",\n",
    "    \"- Employed/ Students\\n\",\n",
    "    \"- Credit Score\\n\",\n",
    "    \"- Income levels\\n\",\n",
    "    \"- Collateral/ security available\\n\",\n",
    "    \"- Other cashflows\\n\",\n",
    "    \"- Secondary applicant/ guarantor\\n\",\n",
    "    \"- Guarantor details \\n\",\n",
    "    \"- Guarantor collateral/ security\\n\",\n",
    "    \"- Debt to income ratio\\n\",\n",
    "    \"- Loan to value ratio\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src = 'https://www.investopedia.com/thmb/xRigJ1OIF1_AyxtvDydgm_D0ASY=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/mortgage-preapproval-4776405_final2-f5fbd4d3d08d4aeeb04cc12fc718ae00.png'\\n\",\n",
    "    \"     width = \\\"750\\\"/>\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"656c81d1\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"<b>II. Brief Background</b>\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. What is the problem?<br>\\n\",\n",
    "    \"This part deals with loan approvals by a regional bank.\\n\",\n",
    "    \"We are trying to predict the eligibility of new loan applicants basis data of those applicants whose loans were approved or rejected.\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"2. Why is it important?<br>\\n\",\n",
    "    \"Loans are a primary source of income for banks, and banks typically have a largr consumer loan vertical which deals with property loans, mortgage loans, car loans and unsecured personal loans.\\n\",\n",
    "    \"There is a certain level of risk/ Risk appetite that each bank has on the basis of which it decideds whether to extend loans or not to their applicants.\\n\",\n",
    "    \"This includes a vast list of parameters, some of which have been listed above.\\n\",\n",
    "    \"It is of utmost importance for the banks to vet the applicants properly to ensure that the loans are being extended to the right people to ensure that the funds extended are recoverable and probability of default is low.\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"3. Who are the key stakeholders?<br>\\n\",\n",
    "    \"In this case, this predictor is beneficial for loan departments of banks, loan agents and loan recovery teams.\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"05e21770\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"<b>III. Data Section</b>\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this case, we have been given information of existing loan applicants along with details of those applicants whose loans have been rejected in the past. Based on this data, we need to predict whether loans of new applicants can be approved.\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"529867b5\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 2. Libraries & Custom Functions <a class=\\\"anchor\\\" id=\\\"libcustfunc\\\"></a>\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"a9947da4\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \">**Pandas:**<br> \\n\",\n",
    "    \"Pandas is an open source Python package that is most widely used for data science/data analysis and machine learning tasks. It is built on top of another package named Numpy, which provides support for multi-dimensional arrays.\\n\",\n",
    "    \"\\n\",\n",
    "    \">**Graphics:**<br>\\n\",\n",
    "    \">- Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\\n\",\n",
    "    \">- Plotly is a Python library for creating interactive, publication-quality visualizations. Plotly not only makes the plots interactive, a functionality missing in matplotlib or seaborn, but also provides a variety of more charts. Plotly allows for a high degree of customization and interactivity, and supports multiple languages. seaborn is a fully open-source Python visualization library built on top of Matplotlib to create visually appealing plots quickly.\\n\",\n",
    "    \"\\n\",\n",
    "    \">**SKLearn:**<br>\\n\",\n",
    "    \"Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. It provides many unsupervised and supervised learning algorithms. It’s built upon NumPy, pandas, and Matplotlib.\\n\",\n",
    "    \"\\n\",\n",
    "    \">>**Logistic Regression:** It is a Machine Learning classifier/ classification algorithm that is used to predict the probability of a categorical dependent variable.\\n\",\n",
    "    \">>>**Accuracy Score:** The accuracy_score function calculates the accuracy score of the predicted output against the true outcome of the target.\\n\",\n",
    "    \"\\n\",\n",
    "    \">**Pickle:**<br>\\n\",\n",
    "    \"The python Pickle module serializes and deserializes Python objects in binary. Pickling is used to store python objects to a file that can be loaded to another program again later.\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"e841be42\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"#Importing packages\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"pd.set_option('display.max_columns', None)\\n\",\n",
    "    \"pd.set_option('max_colwidth', None)\\n\",\n",
    "    \"\\n\",\n",
    "    \"pd.options.display.float_format = '{:.2f}'.format\\n\",\n",
    "    \"\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.neighbors import KNeighborsClassifier\\n\",\n",
    "    \"from sklearn.tree import DecisionTreeClassifier\\n\",\n",
    "    \"from sklearn import tree\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier\\n\",\n",
    "    \"from sklearn.neural_network import MLPClassifier\\n\",\n",
    "    \"\\n\",\n",
    "    \"from sklearn import metrics\\n\",\n",
    "    \"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\\n\",\n",
    "    \"from sklearn.metrics import roc_curve, auc\\n\",\n",
    "    \"\\n\",\n",
    "    \"import matplotlib.pylab as plt\\n\",\n",
    "    \"from sklearn.tree import plot_tree\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"import time\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"id\": \"482d6a32\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Custom function below is used to display performance measures\\n\",\n",
    "    \"# Required inputs are the model name and the two test data objects (X,y)\\n\",\n",
    "    \"def class_perf_measures(model,X_test,y_test, initial_th, final_th, interval):\\n\",\n",
    "    \"    # Create empty lists to store metric values created within loop\\n\",\n",
    "    \"    TP = []\\n\",\n",
    "    \"    TN = []\\n\",\n",
    "    \"    FP = []\\n\",\n",
    "    \"    FN = []\\n\",\n",
    "    \"    TP_per = []\\n\",\n",
    "    \"    TN_per = []\\n\",\n",
    "    \"    FP_per = []\\n\",\n",
    "    \"    FN_per = []\\n\",\n",
    "    \"    Recall = []\\n\",\n",
    "    \"    Precision = []\\n\",\n",
    "    \"    F1 = []\\n\",\n",
    "    \"    Accuracy = []\\n\",\n",
    "    \"    AUC = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create list of probability threshold values to loop over\\n\",\n",
    "    \"    ProbThreshold = np.arange(initial_th,final_th + interval,interval).tolist()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Start loop\\n\",\n",
    "    \"    for i in ProbThreshold:\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Create class assignments given threshold value\\n\",\n",
    "    \"        y_test_pred_class = (model.predict_proba(X_test)[:,1] >= i).astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Append lists with metric values\\n\",\n",
    "    \"        TP_value = metrics.confusion_matrix(y_test, y_test_pred_class)[1,1]\\n\",\n",
    "    \"        TN_value = metrics.confusion_matrix(y_test, y_test_pred_class)[0,0]\\n\",\n",
    "    \"        FP_value = metrics.confusion_matrix(y_test, y_test_pred_class)[0,1]\\n\",\n",
    "    \"        FN_value = metrics.confusion_matrix(y_test, y_test_pred_class)[1,0]\\n\",\n",
    "    \"        TP.append(TP_value)\\n\",\n",
    "    \"        TN.append(TN_value)\\n\",\n",
    "    \"        FP.append(FP_value)\\n\",\n",
    "    \"        FN.append(FN_value)\\n\",\n",
    "    \"        Recall.append(metrics.recall_score(y_test, y_test_pred_class).round(3))\\n\",\n",
    "    \"        Precision.append(metrics.precision_score(y_test, y_test_pred_class).round(3))\\n\",\n",
    "    \"        F1.append(metrics.f1_score(y_test, y_test_pred_class).round(2))\\n\",\n",
    "    \"        Accuracy.append(metrics.accuracy_score(y_test, y_test_pred_class).round(2))\\n\",\n",
    "    \"        TP_per.append(TP_value/(len(y_test)-1)*100)\\n\",\n",
    "    \"        TN_per.append(TN_value/(len(y_test)-1)*100)\\n\",\n",
    "    \"        FP_per.append(FP_value/(len(y_test)-1)*100)\\n\",\n",
    "    \"        FN_per.append(FN_value/(len(y_test)-1)*100)\\n\",\n",
    "    \"        AUC.append(metrics.roc_auc_score(y_test, y_test_pred_class).round(3))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Create dataframe\\n\",\n",
    "    \"    result = pd.DataFrame({\\\"Probability Threshold\\\":ProbThreshold,\\n\",\n",
    "    \"                           \\\"TP\\\":TP,\\n\",\n",
    "    \"                           \\\"TN\\\":TN,\\n\",\n",
    "    \"                           \\\"FP\\\":FP,\\n\",\n",
    "    \"                           \\\"FN\\\":FN,\\n\",\n",
    "    \"                           \\\"TP%\\\": TP_per,\\n\",\n",
    "    \"                           \\\"TN%\\\": TN_per,\\n\",\n",
    "    \"                           \\\"FP%\\\": FP_per,\\n\",\n",
    "    \"                           \\\"FN%\\\": FN_per,\\n\",\n",
    "    \"                           \\\"Precision\\\":Precision,\\n\",\n",
    "    \"                           \\\"Recall\\\":Recall,\\n\",\n",
    "    \"                           \\\"Accuracy\\\": Accuracy,\\n\",\n",
    "    \"                           \\\"F1\\\": F1,\\n\",\n",
    "    \"                           \\\"AUC\\\": AUC\\n\",\n",
    "    \"                          })\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Let's look at our dataframe\\n\",\n",
    "    \"    return result\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"id\": \"79c57c3d\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# This function is used to plot the important features in decision tree\\n\",\n",
    "    \"def plot_feature_importances(data,model):\\n\",\n",
    "    \"    n_features = data.shape[1]\\n\",\n",
    "    \"    fig = plt.figure(figsize=(15,15))\\n\",\n",
    "    \"    plt.barh(range(n_features), model.feature_importances_, align='center')\\n\",\n",
    "    \"    plt.yticks(np.arange(n_features), data.columns)\\n\",\n",
    "    \"    plt.xlabel(\\\"Feature Importance\\\")\\n\",\n",
    "    \"    plt.ylabel(\\\"Feature\\\")\\n\",\n",
    "    \"    plt.ylim(-1, n_features)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"id\": \"7e703349\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# This function is used to consolidate the results of grid search of decision tree\\n\",\n",
    "    \"def decision_tree_grid_search():\\n\",\n",
    "    \"    max_depth = []\\n\",\n",
    "    \"    min_impurity_decrease = []\\n\",\n",
    "    \"    min_samples_split = []    \\n\",\n",
    "    \"    df_output = pd.DataFrame()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    df_output['Accuracy'] = gridSearch.cv_results_.get('mean_test_accuracy')\\n\",\n",
    "    \"    df_output['Recall'] = gridSearch.cv_results_.get('mean_test_recall')\\n\",\n",
    "    \"    df_output['Precision'] = gridSearch.cv_results_.get('mean_test_precision')\\n\",\n",
    "    \"    df_output['AUC'] = gridSearch.cv_results_.get('mean_test_roc_auc')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for result in gridSearch.cv_results_.get('params'):\\n\",\n",
    "    \"        max_depth.append(result.get('max_depth'))\\n\",\n",
    "    \"        min_impurity_decrease.append(result.get('min_impurity_decrease'))\\n\",\n",
    "    \"        min_samples_split.append(result.get('min_samples_split'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    df_output['Max Depth'] = max_depth\\n\",\n",
    "    \"    df_output['Minimum Impurity Decrease'] = min_impurity_decrease\\n\",\n",
    "    \"    df_output['Minimum Samples Split'] = min_samples_split\\n\",\n",
    "    \"    df_output['Model'] = 'Decision Tree'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df_output\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"id\": \"269be202\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"#This funtion is used to add the best performing model for each classifier into a dataframe so compare with other classifiers \\n\",\n",
    "    \"def update_results_df(model,y_test,y_pred,features_used,total_time,df_results,hyperparameters = np.nan):\\n\",\n",
    "    \"    if(is_model_tree_based(model)):\\n\",\n",
    "    \"        keys = ['ccp_alpha', 'max_depth', 'min_impurity_decrease', 'min_samples_split']\\n\",\n",
    "    \"        hyperparameters = {x:hyperparameters[x] for x in keys}\\n\",\n",
    "    \"    elif(is_model_ANN(model)):\\n\",\n",
    "    \"        keys = ['activation', 'hidden_layer_sizes']\\n\",\n",
    "    \"        hyperparameters = {x:hyperparameters[x] for x in keys}\\n\",\n",
    "    \"    print(hyperparameters)\\n\",\n",
    "    \"    results_dict = {\\\"Model\\\": model,\\n\",\n",
    "    \"              \\\"Accuracy\\\": round(metrics.accuracy_score(y_test, y_pred),3),\\n\",\n",
    "    \"              \\\"Recall\\\": round(metrics.recall_score(y_test, y_pred),3),\\n\",\n",
    "    \"              \\\"Precision\\\": round(metrics.precision_score(y_test, y_pred),3),\\n\",\n",
    "    \"              \\\"AUC\\\": round(metrics.roc_auc_score(y_test, y_pred),3),\\n\",\n",
    "    \"              \\\"Total time taken\\\": total_time,\\n\",\n",
    "    \"              \\\"False Negative\\\": confusion_matrix(y_test, y_pred)[1][0],\\n\",\n",
    "    \"              \\\"False Positive\\\": confusion_matrix(y_test, y_pred)[0][1],\\n\",\n",
    "    \"              \\\"Features Used\\\": features_used,\\n\",\n",
    "    \"              \\\"Hyperparameters\\\": str(hyperparameters)}\\n\",\n",
    "    \"    df_results = pd.concat([df_results, pd.DataFrame(results_dict,index=[0])], ignore_index=True)\\n\",\n",
    "    \"    return df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"id\": \"34351d62\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def is_model_tree_based(model):\\n\",\n",
    "    \"    return (\\\"Decision tree\\\" in model) | (\\\"Random Forest\\\" in model)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def is_model_ANN(model):\\n\",\n",
    "    \"    return \\\"Neural Nets\\\" in model\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"a88a604a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 3. Data Exploration and Data Prep <a class=\\\"anchor\\\" id=\\\"dataexp\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"id\": \"28abb418\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>approved</th>\\n\",\n",
    "       \"      <th>gender</th>\\n\",\n",
    "       \"      <th>age</th>\\n\",\n",
    "       \"      <th>debt</th>\\n\",\n",
    "       \"      <th>married</th>\\n\",\n",
    "       \"      <th>bank_customer</th>\\n\",\n",
    "       \"      <th>emp_industrial</th>\\n\",\n",
    "       \"      <th>emp_materials</th>\\n\",\n",
    "       \"      <th>emp_consumer_services</th>\\n\",\n",
    "       \"      <th>emp_healthcare</th>\\n\",\n",
    "       \"      <th>emp_financials</th>\\n\",\n",
    "       \"      <th>emp_utilities</th>\\n\",\n",
    "       \"      <th>emp_education</th>\\n\",\n",
    "       \"      <th>ethnicity_white</th>\\n\",\n",
    "       \"      <th>ethnicity_black</th>\\n\",\n",
    "       \"      <th>ethnicity_latino</th>\\n\",\n",
    "       \"      <th>ethnicity_asian</th>\\n\",\n",
    "       \"      <th>ethnicity_other</th>\\n\",\n",
    "       \"      <th>years_employed</th>\\n\",\n",
    "       \"      <th>prior_default</th>\\n\",\n",
    "       \"      <th>employed</th>\\n\",\n",
    "       \"      <th>credit_score</th>\\n\",\n",
    "       \"      <th>drivers_license</th>\\n\",\n",
    "       \"      <th>citizen_bybirth</th>\\n\",\n",
    "       \"      <th>citizen_other</th>\\n\",\n",
    "       \"      <th>citizen_temporary</th>\\n\",\n",
    "       \"      <th>Income</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>685</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>21.08</td>\\n\",\n",
    "       \"      <td>10.09</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1.25</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>686</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>22.67</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>2.00</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>394</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>687</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>25.25</td>\\n\",\n",
    "       \"      <td>13.50</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>2.00</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>688</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>17.92</td>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0.04</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>750</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>689</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>35.00</td>\\n\",\n",
    "       \"      <td>3.38</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>8.29</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"     approved  gender   age  debt  married  bank_customer  emp_industrial  \\\\\\n\",\n",
    "       \"685         0       1 21.08 10.09        0              0               0   \\n\",\n",
    "       \"686         0       0 22.67  0.75        1              1               1   \\n\",\n",
    "       \"687         0       0 25.25 13.50        0              0               0   \\n\",\n",
    "       \"688         0       1 17.92  0.20        1              1               0   \\n\",\n",
    "       \"689         0       1 35.00  3.38        1              1               1   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     emp_materials  emp_consumer_services  emp_healthcare  emp_financials  \\\\\\n\",\n",
    "       \"685              0                      0               0               0   \\n\",\n",
    "       \"686              0                      0               0               0   \\n\",\n",
    "       \"687              0                      0               1               0   \\n\",\n",
    "       \"688              0                      1               0               0   \\n\",\n",
    "       \"689              0                      0               0               0   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     emp_utilities  emp_education  ethnicity_white  ethnicity_black  \\\\\\n\",\n",
    "       \"685              0              1                0                1   \\n\",\n",
    "       \"686              0              0                1                0   \\n\",\n",
    "       \"687              0              0                0                0   \\n\",\n",
    "       \"688              0              0                1                0   \\n\",\n",
    "       \"689              0              0                0                1   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     ethnicity_latino  ethnicity_asian  ethnicity_other  years_employed  \\\\\\n\",\n",
    "       \"685                 0                0                0            1.25   \\n\",\n",
    "       \"686                 0                0                0            2.00   \\n\",\n",
    "       \"687                 1                0                0            2.00   \\n\",\n",
    "       \"688                 0                0                0            0.04   \\n\",\n",
    "       \"689                 0                0                0            8.29   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     prior_default  employed  credit_score  drivers_license  citizen_bybirth  \\\\\\n\",\n",
    "       \"685              0         0             0                0                1   \\n\",\n",
    "       \"686              0         1             2                1                1   \\n\",\n",
    "       \"687              0         1             1                1                1   \\n\",\n",
    "       \"688              0         0             0                0                1   \\n\",\n",
    "       \"689              0         0             0                1                1   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     citizen_other  citizen_temporary  Income  \\n\",\n",
    "       \"685              0                  0       0  \\n\",\n",
    "       \"686              0                  0     394  \\n\",\n",
    "       \"687              0                  0       1  \\n\",\n",
    "       \"688              0                  0     750  \\n\",\n",
    "       \"689              0                  0       0  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 7,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"#Reading in the data\\n\",\n",
    "    \"\\n\",\n",
    "    \"df = pd.read_csv('Loan_Details.csv')\\n\",\n",
    "    \"df.tail()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"id\": \"2638d569\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# df = df.rename(columns={'approved':'Approval_Status', 'gender':'Gender', 'age':'Age', 'debt':'Debt', 'married':'Married', 'bank_customer':'Existing_Customer'})\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 9,\n",
    "   \"id\": \"037ccae5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df.drop_duplicates(inplace = True)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 10,\n",
    "   \"id\": \"727704d0\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>approved</th>\\n\",\n",
    "       \"      <th>gender</th>\\n\",\n",
    "       \"      <th>age</th>\\n\",\n",
    "       \"      <th>debt</th>\\n\",\n",
    "       \"      <th>married</th>\\n\",\n",
    "       \"      <th>bank_customer</th>\\n\",\n",
    "       \"      <th>emp_industrial</th>\\n\",\n",
    "       \"      <th>emp_materials</th>\\n\",\n",
    "       \"      <th>emp_consumer_services</th>\\n\",\n",
    "       \"      <th>emp_healthcare</th>\\n\",\n",
    "       \"      <th>emp_financials</th>\\n\",\n",
    "       \"      <th>emp_utilities</th>\\n\",\n",
    "       \"      <th>emp_education</th>\\n\",\n",
    "       \"      <th>ethnicity_white</th>\\n\",\n",
    "       \"      <th>ethnicity_black</th>\\n\",\n",
    "       \"      <th>ethnicity_latino</th>\\n\",\n",
    "       \"      <th>ethnicity_asian</th>\\n\",\n",
    "       \"      <th>ethnicity_other</th>\\n\",\n",
    "       \"      <th>years_employed</th>\\n\",\n",
    "       \"      <th>prior_default</th>\\n\",\n",
    "       \"      <th>employed</th>\\n\",\n",
    "       \"      <th>credit_score</th>\\n\",\n",
    "       \"      <th>drivers_license</th>\\n\",\n",
    "       \"      <th>citizen_bybirth</th>\\n\",\n",
    "       \"      <th>citizen_other</th>\\n\",\n",
    "       \"      <th>citizen_temporary</th>\\n\",\n",
    "       \"      <th>Income</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>685</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>21.08</td>\\n\",\n",
    "       \"      <td>10.09</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1.25</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>686</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>22.67</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>2.00</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>394</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>687</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>25.25</td>\\n\",\n",
    "       \"      <td>13.50</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>2.00</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>688</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>17.92</td>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0.04</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>750</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>689</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>35.00</td>\\n\",\n",
    "       \"      <td>3.38</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>8.29</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"     approved  gender   age  debt  married  bank_customer  emp_industrial  \\\\\\n\",\n",
    "       \"685         0       1 21.08 10.09        0              0               0   \\n\",\n",
    "       \"686         0       0 22.67  0.75        1              1               1   \\n\",\n",
    "       \"687         0       0 25.25 13.50        0              0               0   \\n\",\n",
    "       \"688         0       1 17.92  0.20        1              1               0   \\n\",\n",
    "       \"689         0       1 35.00  3.38        1              1               1   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     emp_materials  emp_consumer_services  emp_healthcare  emp_financials  \\\\\\n\",\n",
    "       \"685              0                      0               0               0   \\n\",\n",
    "       \"686              0                      0               0               0   \\n\",\n",
    "       \"687              0                      0               1               0   \\n\",\n",
    "       \"688              0                      1               0               0   \\n\",\n",
    "       \"689              0                      0               0               0   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     emp_utilities  emp_education  ethnicity_white  ethnicity_black  \\\\\\n\",\n",
    "       \"685              0              1                0                1   \\n\",\n",
    "       \"686              0              0                1                0   \\n\",\n",
    "       \"687              0              0                0                0   \\n\",\n",
    "       \"688              0              0                1                0   \\n\",\n",
    "       \"689              0              0                0                1   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     ethnicity_latino  ethnicity_asian  ethnicity_other  years_employed  \\\\\\n\",\n",
    "       \"685                 0                0                0            1.25   \\n\",\n",
    "       \"686                 0                0                0            2.00   \\n\",\n",
    "       \"687                 1                0                0            2.00   \\n\",\n",
    "       \"688                 0                0                0            0.04   \\n\",\n",
    "       \"689                 0                0                0            8.29   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     prior_default  employed  credit_score  drivers_license  citizen_bybirth  \\\\\\n\",\n",
    "       \"685              0         0             0                0                1   \\n\",\n",
    "       \"686              0         1             2                1                1   \\n\",\n",
    "       \"687              0         1             1                1                1   \\n\",\n",
    "       \"688              0         0             0                0                1   \\n\",\n",
    "       \"689              0         0             0                1                1   \\n\",\n",
    "       \"\\n\",\n",
    "       \"     citizen_other  citizen_temporary  Income  \\n\",\n",
    "       \"685              0                  0       0  \\n\",\n",
    "       \"686              0                  0     394  \\n\",\n",
    "       \"687              0                  0       1  \\n\",\n",
    "       \"688              0                  0     750  \\n\",\n",
    "       \"689              0                  0       0  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 10,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df.tail()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 11,\n",
    "   \"id\": \"12c20380\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"predictors = ['debt', 'bank_customer',\\n\",\n",
    "    \"       'emp_industrial', 'emp_materials', 'emp_consumer_services',\\n\",\n",
    "    \"       'emp_healthcare', 'emp_financials', 'emp_utilities', 'emp_education',\\n\",\n",
    "    \"       'years_employed', 'prior_default', 'employed', 'credit_score', 'drivers_license', 'Income']\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 12,\n",
    "   \"id\": \"0395b1df\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"target = ['approved']\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 13,\n",
    "   \"id\": \"013d1db0\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X = df[predictors]\\n\",\n",
    "    \"y = df[target]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 14,\n",
    "   \"id\": \"15f225c9\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=10)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 15,\n",
    "   \"id\": \"48392130\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Normalize predictor variables using Z-Scores\\n\",\n",
    "    \"# Use means and standard deviations of features as calculated in the training data\\n\",\n",
    "    \"# New values are centered at 0.  Values reflect the number of standard deviations\\n\",\n",
    "    \"# each record is above or below the mean.\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_means = X_train.mean()\\n\",\n",
    "    \"features_std = X_train.std()\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_train = (X_train - features_means)/features_std\\n\",\n",
    "    \"X_test = (X_test - features_means)/features_std\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 16,\n",
    "   \"id\": \"e24f9fbe\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_results = pd.DataFrame()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"0bcb9508\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Understanding the Business Problem<a class=\\\"anchor\\\" id=\\\"busprob\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\\n\",\n",
    "    \"\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"Before we get to actual model building, it is important that we understand the business problem and the cost of misclassification\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"<b> Confusion Matrix\\n\",\n",
    "    \"    \\n\",\n",
    "    \"<img src = 'https://oopy.lazyrockets.com/api/v2/notion/image?src=https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa7fb3ffc-5c0e-4db5-89c6-021994823e01%2FUntitled.png&blockId=d5474d00-6501-48b7-a9a1-59d5bbb640d8'\\n\",\n",
    "    \"     width = \\\"400\\\"\\n\",\n",
    "    \"     height = \\\"500\\\"/>\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src = 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/1200px-Precisionrecall.svg.png'\\n\",\n",
    "    \"     width = \\\"300\\\"\\n\",\n",
    "    \"     height = \\\"400\\\"\\n\",\n",
    "    \"     align = \\\"right\\\"/>    \\n\",\n",
    "    \"\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"    <br>\\n\",\n",
    "    \"    <br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"About the Confusion Matrix:\\n\",\n",
    "    \"- <b>Recall/ True Positive Rate / Sensitivity</b>: Sensitivity tells us what proportion of the positive class got correctly classified\\n\",\n",
    "    \"- <b>False Negative Rate</b>: FNR tells us what proportion of the positive class got incorrectly classified by the classifier.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Higher TPR and lower FNR is desirable since we want to correctly classify the positive class\\n\",\n",
    "    \"    \\n\",\n",
    "    \"- <b>True Negative Rate/ Specificity</b>: Tells us what proportion of the negative class got correctly classified  \\n\",\n",
    "    \"- <b>False Positive Rate</b>: FPR tells us what proportion of the negative class got incorrectly classified by the classifier\\n\",\n",
    "    \"\\n\",\n",
    "    \"Higher TNR and a lower FPR is desirable since we want to correctly classify the negative class.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"<img src = 'https://miro.medium.com/max/676/1*k6qWU7kXeCfk2KK2y3Cysg.png'/>\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"<b>In this particular case where the target variable is loan approved, we can say that:</b>\\n\",\n",
    "    \"- Recall/ True Positive would be that the loan is approved.\\n\",\n",
    "    \"- Therefore, False Positive would be that the loan is incorrectly predicted to be approved when in fact it has been rejected.\\n\",\n",
    "    \"- True Negative would be the number of loan applications predicted to be rejected/ not approved.\\n\",\n",
    "    \"- False Negative would be the number of loan applications predicted to be rejected, but were in fact approved.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The following factors can be considered to determine what is costlier for the business:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"<b>1. False Positive:</b>\\n\",\n",
    "    \"When a loan is predicted as approved but in reality has been rejected.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<p>Here, what ensues is as follows:</p>\\n\",\n",
    "    \"- Loan predicted to be approved but when it is rejected means that the model is suggesting a risky loan to be approved<br>\\n\",\n",
    "    \"- This is not advisable as selection of customers and approval of applicants is very important for the health of the bank's financials, and this can result in giving funds to the wrong person<br>\\n\",\n",
    "    \"- Repayment capability can be questioned, and probability of recovery of funds is lowered<br>\\n\",\n",
    "    \"- Internal recovery teams can question the business team and report incorrect approvals of loans\\n\",\n",
    "    \"- Regulatory bodies can question the authenticity of the approvals and can initiate a bank wide audit to look into the past approvals<br>\\n\",\n",
    "    \"- This in turn can damage the reputation of the bank and the credit team and business team may face backlash from the bank consumers and shareholders can demand answers<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"<b>2. False Negative:</b>\\n\",\n",
    "    \"When a loan application is predicted to be not approved when it has been\\n\",\n",
    "    \"\\n\",\n",
    "    \"<p>Here, what ensues is as follows:</p>\\n\",\n",
    "    \"- Loss of business and loss of future (interest) income may happen<br>\\n\",\n",
    "    \"- There can be some level of bad repuation spread if good applicants are regularly being turned away, and the bank may lose out on more good clients based on word-of-mouth from rejected customers<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"<b>Basis above points, we can see False Positive is more detrimental to the bank, and would cause tangible and intangible harm to the company, along increase in defaults and troubles with the regulatory body</b>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Therefore, we should opt for a model which can help us minimize False Positive cases considerably and gives good overall Accuracy, while also limiting the False Negative cases to the extent possible.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e23ac039\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Building the Models:<a class=\\\"anchor\\\" id=\\\"maclrn\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"- [X] [Logistic Model](#first-model)\\n\",\n",
    "    \"- [X] [kNN](#second-model)\\n\",\n",
    "    \"- [X] [Decision Trees](#third-model)\\n\",\n",
    "    \"- [X] [Random Forest](#fourth-model)\\n\",\n",
    "    \"- [X] [Artificial Neural Network](#fifth-model)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"ab607a62\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.1: Logistic Regression<a class=\\\"anchor\\\" id=\\\"first-model\\\"></a>\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"*  [Go to ML Models](#maclrn)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\\n\",\n",
    "    \"\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Logistic regression is used for classification and predictive analysis. It is a supervised machine learning tool\\n\",\n",
    "    \"It estimates the probability of an event occurring, based on given dataset of independent variables.\\n\",\n",
    "    \"\\n\",\n",
    "    \"There are 3 types of logistic regression models as mentioned below:\\n\",\n",
    "    \"(source: https://www.ibm.com/topics/logistic-regression)\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Binary logistic regression:\\n\",\n",
    "    \"The resource allocation problem is of the binary type of logistic regression model, as the dependent variable  is dichotomous in nature, i.e the outcome can only be binary (0 or 1).\\n\",\n",
    "    \"In this case, we are trying to predict whether the task has been completed (1) or if it has not been completed (0).\\n\",\n",
    "    \"This type is generally most common for binary classification.\\n\",\n",
    "    \"\\n\",\n",
    "    \"2. Multinomial logistic regression:\\n\",\n",
    "    \"Here, the dependent variable has 3 or more possible outcomes, however there is no specified order.\\n\",\n",
    "    \"\\n\",\n",
    "    \"3. Ordinal logisitc regression:\\n\",\n",
    "    \"This is similar to multinomial type where the dependent variable has 3 or more possible outcomes, however, here the values have a defined order.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 17,\n",
    "   \"id\": \"5b243bad\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"start_time = time.time()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 18,\n",
    "   \"id\": \"6df41401\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"lr_model = LogisticRegression(max_iter=1000)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 19,\n",
    "   \"id\": \"bec88924\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"lr_model.fit(X_train, y_train.values.ravel())\\n\",\n",
    "    \"\\n\",\n",
    "    \"y_train_pred = lr_model.predict(X_train)\\n\",\n",
    "    \"y_test_pred = lr_model.predict(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_used = len(lr_model.coef_[0][(lr_model.coef_[0] > 0.01) | (lr_model.coef_[0] < -0.01)]) - 1\\n\",\n",
    "    \"features_used = np.where(features_used == -1, 0, features_used)\\n\",\n",
    "    \"\\n\",\n",
    "    \"total_time = time.time() - start_time\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 20,\n",
    "   \"id\": \"4100d4bc\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"nan\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"model = \\\"Logistic Regression\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,y_test_pred,features_used,total_time,df_results)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 21,\n",
    "   \"id\": \"46c4e4c1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Model</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"      <th>Total time taken</th>\\n\",\n",
    "       \"      <th>False Negative</th>\\n\",\n",
    "       \"      <th>False Positive</th>\\n\",\n",
    "       \"      <th>Features Used</th>\\n\",\n",
    "       \"      <th>Hyperparameters</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Logistic Regression</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"                 Model  Accuracy  Recall  Precision  AUC  Total time taken  \\\\\\n\",\n",
    "       \"0  Logistic Regression      0.88    0.86       0.88 0.88              0.03   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   False Negative  False Positive  Features Used Hyperparameters  \\n\",\n",
    "       \"0              13              11             14             nan  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 21,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 22,\n",
    "   \"id\": \"ee281620\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\\n\",\n",
    "      \"  _warn_prf(average, modifier, msg_start, len(result))\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>0.62</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.10</td>\\n\",\n",
    "       \"      <td>93</td>\\n\",\n",
    "       \"      <td>65</td>\\n\",\n",
    "       \"      <td>48</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>45.15</td>\\n\",\n",
    "       \"      <td>31.55</td>\\n\",\n",
    "       \"      <td>23.30</td>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>0.66</td>\\n\",\n",
    "       \"      <td>0.99</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>93</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>28</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>45.15</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>13.59</td>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.99</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>95</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>46.12</td>\\n\",\n",
    "       \"      <td>8.74</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>98</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>47.57</td>\\n\",\n",
    "       \"      <td>7.28</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>81</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>39.32</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>6.31</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.60</td>\\n\",\n",
    "       \"      <td>77</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>37.38</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>8.25</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>71</td>\\n\",\n",
    "       \"      <td>105</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>34.47</td>\\n\",\n",
    "       \"      <td>50.97</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>65</td>\\n\",\n",
    "       \"      <td>105</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>29</td>\\n\",\n",
    "       \"      <td>31.55</td>\\n\",\n",
    "       \"      <td>50.97</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>14.08</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.69</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>43</td>\\n\",\n",
    "       \"      <td>110</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>51</td>\\n\",\n",
    "       \"      <td>20.87</td>\\n\",\n",
    "       \"      <td>53.40</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>24.76</td>\\n\",\n",
    "       \"      <td>0.94</td>\\n\",\n",
    "       \"      <td>0.46</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.61</td>\\n\",\n",
    "       \"      <td>0.71</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.55</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN   FP  FN   TP%   TN%   FP%   FN%  \\\\\\n\",\n",
    "       \"0                    0.00  94    0  113   0 45.63  0.00 54.85  0.00   \\n\",\n",
    "       \"1                    0.10  93   65   48   1 45.15 31.55 23.30  0.49   \\n\",\n",
    "       \"2                    0.20  93   85   28   1 45.15 41.26 13.59  0.49   \\n\",\n",
    "       \"3                    0.30  91   95   18   3 44.17 46.12  8.74  1.46   \\n\",\n",
    "       \"4                    0.40  85   98   15   9 41.26 47.57  7.28  4.37   \\n\",\n",
    "       \"5                    0.50  81  102   11  13 39.32 49.51  5.34  6.31   \\n\",\n",
    "       \"6                    0.60  77  102   11  17 37.38 49.51  5.34  8.25   \\n\",\n",
    "       \"7                    0.70  71  105    8  23 34.47 50.97  3.88 11.17   \\n\",\n",
    "       \"8                    0.80  65  105    8  29 31.55 50.97  3.88 14.08   \\n\",\n",
    "       \"9                    0.90  43  110    3  51 20.87 53.40  1.46 24.76   \\n\",\n",
    "       \"10                   1.00   0  113    0  94  0.00 54.85  0.00 45.63   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Precision  Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0        0.45    1.00      0.45 0.62 0.50  \\n\",\n",
    "       \"1        0.66    0.99      0.76 0.79 0.78  \\n\",\n",
    "       \"2        0.77    0.99      0.86 0.87 0.87  \\n\",\n",
    "       \"3        0.83    0.97      0.90 0.90 0.90  \\n\",\n",
    "       \"4        0.85    0.90      0.88 0.88 0.89  \\n\",\n",
    "       \"5        0.88    0.86      0.88 0.87 0.88  \\n\",\n",
    "       \"6        0.88    0.82      0.86 0.85 0.86  \\n\",\n",
    "       \"7        0.90    0.76      0.85 0.82 0.84  \\n\",\n",
    "       \"8        0.89    0.69      0.82 0.78 0.81  \\n\",\n",
    "       \"9        0.94    0.46      0.74 0.61 0.71  \\n\",\n",
    "       \"10       0.00    0.00      0.55 0.00 0.50  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 22,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"class_perf_measures(lr_model, X_test, y_test,0,1,0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"a38de157\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Here, we should choose the  probability threshold of 0.70 as it minimizes the False Positive rate (<4%) with a high Precision of 90%. Overall accuracy of the model is also observed to be pretty good at 85%\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 23,\n",
    "   \"id\": \"b5d4270c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"\\n\",\n",
    "    \"logreg_threshold = 0.7\\n\",\n",
    "    \"\\n\",\n",
    "    \"logreg_pred_2 = (lr_model.predict_proba(X_test)[:,1] >= logreg_threshold).astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_used = len(lr_model.coef_[0][(lr_model.coef_[0] > 0.01) | (lr_model.coef_[0] < -0.01)]) - 1\\n\",\n",
    "    \"features_used = np.where(features_used == -1, 0, features_used)\\n\",\n",
    "    \"\\n\",\n",
    "    \"total_time = time.time() - start_time\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 24,\n",
    "   \"id\": \"23b34fe5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"nan\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"model = \\\"Logistic Regression for Selected Features\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,y_test_pred,features_used,total_time,df_results)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 25,\n",
    "   \"id\": \"2bf21dc5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Model</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"      <th>Total time taken</th>\\n\",\n",
    "       \"      <th>False Negative</th>\\n\",\n",
    "       \"      <th>False Positive</th>\\n\",\n",
    "       \"      <th>Features Used</th>\\n\",\n",
    "       \"      <th>Hyperparameters</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Logistic Regression</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>Logistic Regression for Selected Features</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"                                       Model  Accuracy  Recall  Precision  \\\\\\n\",\n",
    "       \"0                        Logistic Regression      0.88    0.86       0.88   \\n\",\n",
    "       \"1  Logistic Regression for Selected Features      0.88    0.86       0.88   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   AUC  Total time taken  False Negative  False Positive  Features Used  \\\\\\n\",\n",
    "       \"0 0.88              0.03              13              11             14   \\n\",\n",
    "       \"1 0.88              0.00              13              11             14   \\n\",\n",
    "       \"\\n\",\n",
    "       \"  Hyperparameters  \\n\",\n",
    "       \"0             nan  \\n\",\n",
    "       \"1             nan  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 25,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"4372e82a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.2: kNN Classification<a class=\\\"anchor\\\" id=\\\"second-model\\\"></a>\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"*  [Go to ML Models](#maclrn)\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\\n\",\n",
    "    \"\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"The k-Nearest Neighbours algorithm is a supervised machine learning algorithm which is used for both classification (discrete values) and regression questions (real values).\\n\",\n",
    "    \"\\n\",\n",
    "    \"The kNN algorithm assumes that similar things exist in close proximity and this proximity is calculated by calculating the distance between two points (generally using Euclidean distance)\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png'/>\\n\",\n",
    "    \"\\n\",\n",
    "    \"> How kNN works:\\n\",\n",
    "    \"(source: https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761(\\n\",\n",
    "    \"> 1. Load the data\\n\",\n",
    "    \"2. Initialize K to your chosen number of neighbors\\n\",\n",
    "    \"3. For each example in the data\\n\",\n",
    "    \"3.1 Calculate the distance between the query example and the current example from the data.\\n\",\n",
    "    \"3.2 Add the distance and the index of the example to an ordered collection\\n\",\n",
    "    \"4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\\n\",\n",
    "    \"5. Pick the first K entries from the sorted collection\\n\",\n",
    "    \"6. Get the labels of the selected K entries\\n\",\n",
    "    \"7. If regression, return the mean of the K labels\\n\",\n",
    "    \"8. If classification, return the mode of the K labels\\n\",\n",
    "    \"\\n\",\n",
    "    \"Choosing the right value of k:\\n\",\n",
    "    \"- Low values of k (say k = 1) will result in over-fitting\\n\",\n",
    "    \"- High values of k will result in under-fitting\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 26,\n",
    "   \"id\": \"34b18237\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>k</th>\\n\",\n",
    "       \"      <th>Training Accuracy</th>\\n\",\n",
    "       \"      <th>Training Precision</th>\\n\",\n",
    "       \"      <th>Training Recall</th>\\n\",\n",
    "       \"      <th>Test Accuracy</th>\\n\",\n",
    "       \"      <th>Test Precision</th>\\n\",\n",
    "       \"      <th>Test Recall</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>0.91</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.72</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.71</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>11</th>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>12</th>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>13</th>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>14</th>\\n\",\n",
    "       \"      <td>29</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>15</th>\\n\",\n",
    "       \"      <td>31</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>16</th>\\n\",\n",
    "       \"      <td>33</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>17</th>\\n\",\n",
    "       \"      <td>35</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>18</th>\\n\",\n",
    "       \"      <td>37</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.72</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>19</th>\\n\",\n",
    "       \"      <td>39</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>20</th>\\n\",\n",
    "       \"      <td>41</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.72</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>21</th>\\n\",\n",
    "       \"      <td>43</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>22</th>\\n\",\n",
    "       \"      <td>45</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>23</th>\\n\",\n",
    "       \"      <td>47</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>24</th>\\n\",\n",
    "       \"      <td>49</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"     k  Training Accuracy  Training Precision  Training Recall  Test Accuracy  \\\\\\n\",\n",
    "       \"0    1               1.00                1.00             1.00           0.81   \\n\",\n",
    "       \"1    3               0.91                0.89             0.90           0.81   \\n\",\n",
    "       \"2    5               0.89                0.89             0.85           0.84   \\n\",\n",
    "       \"3    7               0.88                0.88             0.84           0.83   \\n\",\n",
    "       \"4    9               0.87                0.86             0.84           0.83   \\n\",\n",
    "       \"5   11               0.87                0.87             0.82           0.82   \\n\",\n",
    "       \"6   13               0.86                0.88             0.79           0.83   \\n\",\n",
    "       \"7   15               0.87                0.89             0.79           0.81   \\n\",\n",
    "       \"8   17               0.86                0.88             0.78           0.83   \\n\",\n",
    "       \"9   19               0.85                0.88             0.76           0.84   \\n\",\n",
    "       \"10  21               0.84                0.88             0.75           0.84   \\n\",\n",
    "       \"11  23               0.84                0.87             0.76           0.84   \\n\",\n",
    "       \"12  25               0.83                0.86             0.75           0.82   \\n\",\n",
    "       \"13  27               0.83                0.86             0.75           0.83   \\n\",\n",
    "       \"14  29               0.84                0.87             0.75           0.82   \\n\",\n",
    "       \"15  31               0.83                0.86             0.75           0.81   \\n\",\n",
    "       \"16  33               0.83                0.85             0.74           0.81   \\n\",\n",
    "       \"17  35               0.82                0.83             0.73           0.81   \\n\",\n",
    "       \"18  37               0.82                0.84             0.72           0.80   \\n\",\n",
    "       \"19  39               0.82                0.83             0.73           0.82   \\n\",\n",
    "       \"20  41               0.82                0.84             0.72           0.82   \\n\",\n",
    "       \"21  43               0.82                0.84             0.74           0.81   \\n\",\n",
    "       \"22  45               0.82                0.84             0.73           0.80   \\n\",\n",
    "       \"23  47               0.82                0.84             0.74           0.81   \\n\",\n",
    "       \"24  49               0.83                0.85             0.73           0.81   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Test Precision  Test Recall  \\n\",\n",
    "       \"0             0.83         0.73  \\n\",\n",
    "       \"1             0.82         0.76  \\n\",\n",
    "       \"2             0.87         0.77  \\n\",\n",
    "       \"3             0.85         0.76  \\n\",\n",
    "       \"4             0.85         0.76  \\n\",\n",
    "       \"5             0.86         0.72  \\n\",\n",
    "       \"6             0.86         0.73  \\n\",\n",
    "       \"7             0.85         0.71  \\n\",\n",
    "       \"8             0.86         0.74  \\n\",\n",
    "       \"9             0.86         0.77  \\n\",\n",
    "       \"10            0.86         0.77  \\n\",\n",
    "       \"11            0.84         0.79  \\n\",\n",
    "       \"12            0.83         0.77  \\n\",\n",
    "       \"13            0.83         0.78  \\n\",\n",
    "       \"14            0.83         0.76  \\n\",\n",
    "       \"15            0.82         0.76  \\n\",\n",
    "       \"16            0.82         0.76  \\n\",\n",
    "       \"17            0.81         0.74  \\n\",\n",
    "       \"18            0.80         0.74  \\n\",\n",
    "       \"19            0.82         0.77  \\n\",\n",
    "       \"20            0.81         0.78  \\n\",\n",
    "       \"21            0.81         0.77  \\n\",\n",
    "       \"22            0.80         0.76  \\n\",\n",
    "       \"23            0.81         0.74  \\n\",\n",
    "       \"24            0.82         0.73  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 26,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for k in np.arange(1,51,2):\\n\",\n",
    "    \"    knn_model = KNeighborsClassifier(n_neighbors=k)\\n\",\n",
    "    \"    knn_model.fit(X_train,y_train.values.ravel())\\n\",\n",
    "    \"\\n\",\n",
    "    \"    y_train_pred = knn_model.predict(X_train)\\n\",\n",
    "    \"    y_test_pred = knn_model.predict(X_test)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    results.append({'k':k,\\n\",\n",
    "    \"                  'Training Accuracy':metrics.accuracy_score(y_train,y_train_pred).round(2),\\n\",\n",
    "    \"                  'Training Precision':metrics.precision_score(y_train,y_train_pred).round(2),\\n\",\n",
    "    \"                  'Training Recall':metrics.recall_score(y_train,y_train_pred).round(2),\\n\",\n",
    "    \"                  'Test Accuracy':metrics.accuracy_score(y_test,y_test_pred).round(2),\\n\",\n",
    "    \"                  'Test Precision':metrics.precision_score(y_test,y_test_pred).round(2),\\n\",\n",
    "    \"                  'Test Recall':metrics.recall_score(y_test,y_test_pred).round(2)\\n\",\n",
    "    \"                  })\\n\",\n",
    "    \"\\n\",\n",
    "    \"results_knn = pd.DataFrame(results)\\n\",\n",
    "    \"results_knn\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 27,\n",
    "   \"id\": \"fc97d92f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"<AxesSubplot:xlabel='k', ylabel='Test Accuracy'>\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 27,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"image/png\": \"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\\n\",\n",
    "      \"text/plain\": [\n",
    "       \"<Figure size 432x288 with 1 Axes>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {\n",
    "      \"needs_background\": \"light\"\n",
    "     },\n",
    "     \"output_type\": \"display_data\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"sns.lineplot(data=results_knn, x='k',y='Test Accuracy')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"0bd67727\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"> **Decision:**<br>\\n\",\n",
    "    \"> Based on the results above, I will set my number of neighbors (k) to 13 as this is the lowest value that achieves the highest accuracy value along with good precision and recall\\n\",\n",
    "    \"\\n\",\n",
    "    \"> **Re-run kNN using the value selected above:**<br>\\n\",\n",
    "    \"> For this final run of kNN, we will produce a table of performance measures across a range of probability threshold values\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 28,\n",
    "   \"id\": \"e5c0ad13\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Training Data Accuracy: 0.86\\n\",\n",
    "      \"Testing Data Accuracy: 0.83\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"knn = KNeighborsClassifier(n_neighbors=13)\\n\",\n",
    "    \"knn.fit(X_train,y_train.values.ravel())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Let's do a quick check to see if we have to worry about problems of overfitting\\n\",\n",
    "    \"# Small values for \\\"k\\\" in kNN tend to lead to overfitting, while large value for \\\"k\\\" in kNN tend to produce underfit models\\n\",\n",
    "    \"y_train_pred_class = knn.predict(X_train)\\n\",\n",
    "    \"y_test_pred_class = knn.predict(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('Training Data Accuracy:', metrics.accuracy_score(y_train,y_train_pred_class).round(2))\\n\",\n",
    "    \"print('Testing Data Accuracy:', metrics.accuracy_score(y_test,y_test_pred_class).round(2))\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 29,\n",
    "   \"id\": \"ed76b06f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"total_time = time.time()-start_time\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 30,\n",
    "   \"id\": \"17dabf61\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\\n\",\n",
    "      \"  _warn_prf(average, modifier, msg_start, len(result))\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>0.62</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.10</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>26</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>12.62</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.52</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.58</td>\\n\",\n",
    "       \"      <td>0.68</td>\\n\",\n",
    "       \"      <td>0.61</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>50</td>\\n\",\n",
    "       \"      <td>63</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>24.27</td>\\n\",\n",
    "       \"      <td>30.58</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.59</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.69</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.71</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>29</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>14.08</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>81</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>39.32</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>10.19</td>\\n\",\n",
    "       \"      <td>6.31</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>69</td>\\n\",\n",
    "       \"      <td>98</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>33.50</td>\\n\",\n",
    "       \"      <td>47.57</td>\\n\",\n",
    "       \"      <td>7.28</td>\\n\",\n",
    "       \"      <td>12.14</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.60</td>\\n\",\n",
    "       \"      <td>60</td>\\n\",\n",
    "       \"      <td>103</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>34</td>\\n\",\n",
    "       \"      <td>29.13</td>\\n\",\n",
    "       \"      <td>50.00</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>16.50</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.64</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>45</td>\\n\",\n",
    "       \"      <td>108</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>49</td>\\n\",\n",
    "       \"      <td>21.84</td>\\n\",\n",
    "       \"      <td>52.43</td>\\n\",\n",
    "       \"      <td>2.43</td>\\n\",\n",
    "       \"      <td>23.79</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.48</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.63</td>\\n\",\n",
    "       \"      <td>0.72</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>111</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>72</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>53.88</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>34.95</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.23</td>\\n\",\n",
    "       \"      <td>0.64</td>\\n\",\n",
    "       \"      <td>0.37</td>\\n\",\n",
    "       \"      <td>0.61</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>88</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>42.72</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.06</td>\\n\",\n",
    "       \"      <td>0.57</td>\\n\",\n",
    "       \"      <td>0.12</td>\\n\",\n",
    "       \"      <td>0.53</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.55</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN   FP  FN   TP%   TN%   FP%   FN%  \\\\\\n\",\n",
    "       \"0                    0.00  94    0  113   0 45.63  0.00 54.85  0.00   \\n\",\n",
    "       \"1                    0.10  94   26   87   0 45.63 12.62 42.23  0.00   \\n\",\n",
    "       \"2                    0.20  92   50   63   2 44.66 24.27 30.58  0.97   \\n\",\n",
    "       \"3                    0.30  84   84   29  10 40.78 40.78 14.08  4.85   \\n\",\n",
    "       \"4                    0.40  81   92   21  13 39.32 44.66 10.19  6.31   \\n\",\n",
    "       \"5                    0.50  69   98   15  25 33.50 47.57  7.28 12.14   \\n\",\n",
    "       \"6                    0.60  60  103   10  34 29.13 50.00  4.85 16.50   \\n\",\n",
    "       \"7                    0.70  45  108    5  49 21.84 52.43  2.43 23.79   \\n\",\n",
    "       \"8                    0.80  22  111    2  72 10.68 53.88  0.97 34.95   \\n\",\n",
    "       \"9                    0.90   6  113    0  88  2.91 54.85  0.00 42.72   \\n\",\n",
    "       \"10                   1.00   0  113    0  94  0.00 54.85  0.00 45.63   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Precision  Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0        0.45    1.00      0.45 0.62 0.50  \\n\",\n",
    "       \"1        0.52    1.00      0.58 0.68 0.61  \\n\",\n",
    "       \"2        0.59    0.98      0.69 0.74 0.71  \\n\",\n",
    "       \"3        0.74    0.89      0.81 0.81 0.82  \\n\",\n",
    "       \"4        0.79    0.86      0.84 0.83 0.84  \\n\",\n",
    "       \"5        0.82    0.73      0.81 0.78 0.80  \\n\",\n",
    "       \"6        0.86    0.64      0.79 0.73 0.78  \\n\",\n",
    "       \"7        0.90    0.48      0.74 0.63 0.72  \\n\",\n",
    "       \"8        0.92    0.23      0.64 0.37 0.61  \\n\",\n",
    "       \"9        1.00    0.06      0.57 0.12 0.53  \\n\",\n",
    "       \"10       0.00    0.00      0.55 0.00 0.50  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 30,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"class_perf_measures(knn_model, X_test, y_test,0,1,0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"dc2f6207\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"> For our KNN model with k=13, we should go with the probability threshold of 0.60 as it minimizes the False Positive rate (<5%) with good Precision of 81%.\\n\",\n",
    "    \"\\n\",\n",
    "    \">We should also keep recall in mind as well as we do not want to deny loan to the people who are actually eligible. This would minimize the profit to the bank. So, keeping all the factors in mind. a threshold of 0.60 seems to be the ideal choice.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 31,\n",
    "   \"id\": \"75cf02eb\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"nan\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"model = \\\"kNN Classification\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,y_test_pred,features_used,total_time,df_results)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 32,\n",
    "   \"id\": \"4e29f6dd\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Model</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"      <th>Total time taken</th>\\n\",\n",
    "       \"      <th>False Negative</th>\\n\",\n",
    "       \"      <th>False Positive</th>\\n\",\n",
    "       \"      <th>Features Used</th>\\n\",\n",
    "       \"      <th>Hyperparameters</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Logistic Regression</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>Logistic Regression for Selected Features</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>kNN Classification</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"                                       Model  Accuracy  Recall  Precision  \\\\\\n\",\n",
    "       \"0                        Logistic Regression      0.88    0.86       0.88   \\n\",\n",
    "       \"1  Logistic Regression for Selected Features      0.88    0.86       0.88   \\n\",\n",
    "       \"2                         kNN Classification      0.81    0.73       0.82   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   AUC  Total time taken  False Negative  False Positive  Features Used  \\\\\\n\",\n",
    "       \"0 0.88              0.03              13              11             14   \\n\",\n",
    "       \"1 0.88              0.00              13              11             14   \\n\",\n",
    "       \"2 0.80              0.82              25              15             14   \\n\",\n",
    "       \"\\n\",\n",
    "       \"  Hyperparameters  \\n\",\n",
    "       \"0             nan  \\n\",\n",
    "       \"1             nan  \\n\",\n",
    "       \"2             nan  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 32,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"febd3f31\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.3 Decision Trees<a class=\\\"anchor\\\" id=\\\"third-model\\\"></a>\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"*  [Go to ML Models](#maclrn)\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\\n\",\n",
    "    \"\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"A decision tree is a map of the possible outcomes of a series of related choices. It allows an individual or organization to weigh possible actions against one another based on their costs, probabilities, and benefits. They can can be used either to drive informal discussion or to map out an algorithm that predicts the best choice mathematically.\\n\",\n",
    "    \"\\n\",\n",
    "    \"A decision tree typically starts with a single node, which branches into possible outcomes. Each of those outcomes leads to additional nodes, which branch off into other possibilities. This gives it a treelike shape.\\n\",\n",
    "    \"\\n\",\n",
    "    \"There are three different types of nodes: chance nodes, decision nodes, and end nodes. A chance node, represented by a circle, shows the probabilities of certain results. A decision node, represented by a square, shows a decision to be made, and an end node shows the final outcome of a decision path.\\n\",\n",
    "    \"\\n\",\n",
    "    \"For more details: https://www.lucidchart.com/pages/decision-tree\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src = 'https://i0.wp.com/why-change.com/wp-content/uploads/2021/11/Decision-Tree-elements-2.png?resize=715%2C450&ssl=1'/>\\n\",\n",
    "    \"\\n\",\n",
    "    \"In Python, Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"<u>Advantages of Decision Trees:</u>\\n\",\n",
    "    \"- Simple to understand and to interpret. Trees can be visualized.\\n\",\n",
    "    \"- Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\\n\",\n",
    "    \"- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\\n\",\n",
    "    \"- Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\\n\",\n",
    "    \"- Able to handle multi-output problems.\\n\",\n",
    "    \"- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\\n\",\n",
    "    \"- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\\n\",\n",
    "    \"- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<u>Disadvantages of Decision Trees:</u>\\n\",\n",
    "    \"- Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\\n\",\n",
    "    \"- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\\n\",\n",
    "    \"- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation.\\n\",\n",
    "    \"- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\\n\",\n",
    "    \"- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\\n\",\n",
    "    \"- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\\n\",\n",
    "    \"\\n\",\n",
    "    \"For more details: https://scikit-learn.org/stable/modules/tree.html\\n\",\n",
    "    \"\\n\",\n",
    "    \"<u>Decision Tree Representation:</u>\\n\",\n",
    "    \"Decision trees classify instances by sorting them down the tree from the root to some leaf node, which provides the classification of the instance. An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute as shown in the above figure. This process is then repeated for the subtree rooted at the new node. \\n\",\n",
    "    \"\\n\",\n",
    "    \"<u>Gini Index:</u>\\n\",\n",
    "    \"Gini Index is a score that evaluates how accurate a split is among the classified groups. Gini index evaluates a score in the range between 0 and 1, where 0 is when all observations belong to one class, and 1 is a random distribution of the elements within classes. In this case, we want to have a Gini index score as low as possible. Gini Index is the evaluation metrics we shall use to evaluate our Decision Tree Model.\\n\",\n",
    "    \"\\n\",\n",
    "    \"For more details: https://www.geeksforgeeks.org/decision-tree/\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 33,\n",
    "   \"id\": \"d6cd14aa\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \\\"▸\\\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \\\"▾\\\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \\\"\\\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \\\"\\\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \\\"\\\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\\\"sk-container-id-1\\\" class=\\\"sk-top-container\\\"><div class=\\\"sk-text-repr-fallback\\\"><pre>DecisionTreeClassifier(max_depth=1, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\\\"sk-container\\\" hidden><div class=\\\"sk-item\\\"><div class=\\\"sk-estimator sk-toggleable\\\"><input class=\\\"sk-toggleable__control sk-hidden--visually\\\" id=\\\"sk-estimator-id-1\\\" type=\\\"checkbox\\\" checked><label for=\\\"sk-estimator-id-1\\\" class=\\\"sk-toggleable__label sk-toggleable__label-arrow\\\">DecisionTreeClassifier</label><div class=\\\"sk-toggleable__content\\\"><pre>DecisionTreeClassifier(max_depth=1, random_state=0)</pre></div></div></div></div></div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"DecisionTreeClassifier(max_depth=1, random_state=0)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 33,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"decision_tree = DecisionTreeClassifier(random_state=0,max_depth=1)\\n\",\n",
    "    \"decision_tree.fit(X,y)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 34,\n",
    "   \"id\": \"5b0e58f7\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# fig = plt.figure(figsize=(10,8))\\n\",\n",
    "    \"# tree.plot_tree(decision_tree,\\n\",\n",
    "    \"#                    feature_names=X.columns,  \\n\",\n",
    "    \"#                    class_names=['Not Approved','Approved'],\\n\",\n",
    "    \"#                    filled=True,\\n\",\n",
    "    \"#                    fontsize=12\\n\",\n",
    "    \"#                   )\\n\",\n",
    "    \"# fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 35,\n",
    "   \"id\": \"48cfc91d\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Impurity Decrease: 0.01 | Accuracy: 0.841 | Precision: 0.886 | AUC: 0.833\\n\",\n",
    "      \"Impurity Decrease: 0.001 | Accuracy: 0.85 | Precision: 0.846 | AUC: 0.848\\n\",\n",
    "      \"Impurity Decrease: 0.0001 | Accuracy: 0.85 | Precision: 0.846 | AUC: 0.848\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"impurity_list = [0.01,0.001,0.0001]\\n\",\n",
    "    \"for impurity in impurity_list:\\n\",\n",
    "    \"    decision_tree = DecisionTreeClassifier(max_depth=10,\\n\",\n",
    "    \"                                       min_samples_split=15,\\n\",\n",
    "    \"                                       min_impurity_decrease=impurity,\\n\",\n",
    "    \"                                       random_state=0)\\n\",\n",
    "    \"    decision_tree.fit(X_train, y_train)\\n\",\n",
    "    \"    decision_tree_pred = decision_tree.predict(X_test)\\n\",\n",
    "    \"    print(\\\"Impurity Decrease: \\\" + str(impurity) + \\\" |\\\" + \\\" Accuracy: \\\" + str(round(metrics.accuracy_score(y_test, decision_tree_pred),3)) + \\\" |\\\" + \\\" Precision: \\\" + str(round(metrics.precision_score(y_test, decision_tree_pred),3)) + \\\" |\\\" + \\\" AUC: \\\" + str(round(metrics.roc_auc_score(y_test, decision_tree_pred),3)))\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 36,\n",
    "   \"id\": \"ec3c9598\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# fig = plt.figure(figsize=(10,8))\\n\",\n",
    "    \"# tree.plot_tree(decision_tree,\\n\",\n",
    "    \"#                    feature_names=X.columns,  \\n\",\n",
    "    \"#                    class_names=['Not Approved','Approved'],\\n\",\n",
    "    \"#                    filled=True,\\n\",\n",
    "    \"#                    fontsize=12\\n\",\n",
    "    \"#                   )\\n\",\n",
    "    \"# fig.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"ad2052bc\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"- After running the decision tree a few times, I have found that max depth should be greater than 10, min sample size can go below 15 and minimum impurity decrease should be as low as possible thus, I am going for grid search using the above as my hyperparameters\\n\",\n",
    "    \"- We will be using accuracy, recall and precision as scoring parameters\\n\",\n",
    "    \"- We will use gridsearch to find 3 best decision trees, one for accuracy, one for recall and one for AUC\\n\",\n",
    "    \"- Cross validation: 5\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 37,\n",
    "   \"id\": \"d6e2665b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"param_grid = {\\n\",\n",
    "    \"    'max_depth': [10,30],\\n\",\n",
    "    \"    'min_samples_split': [5,15,20],\\n\",\n",
    "    \"    'min_impurity_decrease': [0, 0.0001]\\n\",\n",
    "    \"}\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 38,\n",
    "   \"id\": \"8cbc04ea\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"gridSearch = GridSearchCV(DecisionTreeClassifier(random_state=1), \\n\",\n",
    "    \"                          param_grid, \\n\",\n",
    "    \"                          cv=5,\\n\",\n",
    "    \"                          n_jobs=-1,  #used to marshall all available CPUs\\n\",\n",
    "    \"                          scoring=['accuracy','recall','precision','roc_auc'],\\n\",\n",
    "    \"                          refit='roc_auc'\\n\",\n",
    "    \"                         )\\n\",\n",
    "    \"gridSearch.fit(X_train,y_train)\\n\",\n",
    "    \"best_model_results = pd.DataFrame(gridSearch.cv_results_).loc[gridSearch.best_index_]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"2fd9f381\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"**Observation**\\n\",\n",
    "    \"\\n\",\n",
    "    \"- For decision tree on average the accuracy is 85% whereas Precision is 84%\\n\",\n",
    "    \"- Model is performing good for Minimum Impurity Decrease of 0.001\\n\",\n",
    "    \"- Lets see best performing decision tree and its important features and add the best performing tree in the df_resuls\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 39,\n",
    "   \"id\": \"95cf5dac\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"image/png\": \"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\\n\",\n",
    "      \"text/plain\": [\n",
    "       \"<Figure size 1080x1080 with 1 Axes>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {\n",
    "      \"needs_background\": \"light\"\n",
    "     },\n",
    "     \"output_type\": \"display_data\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"plot_feature_importances(X_train,gridSearch.best_estimator_)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"42d0de96\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"- We have found the best performing decision tree, now lets find for which threshold it is giving optimum resilts\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 40,\n",
    "   \"id\": \"313e636d\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>0.62</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.10</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>26</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>12.62</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>79</td>\\n\",\n",
    "       \"      <td>88</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>38.35</td>\\n\",\n",
    "       \"      <td>42.72</td>\\n\",\n",
    "       \"      <td>12.14</td>\\n\",\n",
    "       \"      <td>7.28</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>89</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>43.20</td>\\n\",\n",
    "       \"      <td>11.65</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>10.19</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>76</td>\\n\",\n",
    "       \"      <td>100</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>36.89</td>\\n\",\n",
    "       \"      <td>48.54</td>\\n\",\n",
    "       \"      <td>6.31</td>\\n\",\n",
    "       \"      <td>8.74</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.60</td>\\n\",\n",
    "       \"      <td>76</td>\\n\",\n",
    "       \"      <td>100</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>36.89</td>\\n\",\n",
    "       \"      <td>48.54</td>\\n\",\n",
    "       \"      <td>6.31</td>\\n\",\n",
    "       \"      <td>8.74</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>70</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>33.98</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>11.65</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>69</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>33.50</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>12.14</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>69</td>\\n\",\n",
    "       \"      <td>107</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>33.50</td>\\n\",\n",
    "       \"      <td>51.94</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>12.14</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>88</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>42.72</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.06</td>\\n\",\n",
    "       \"      <td>0.57</td>\\n\",\n",
    "       \"      <td>0.12</td>\\n\",\n",
    "       \"      <td>0.53</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN   FP  FN   TP%   TN%   FP%   FN%  \\\\\\n\",\n",
    "       \"0                    0.00  94    0  113   0 45.63  0.00 54.85  0.00   \\n\",\n",
    "       \"1                    0.10  84   87   26  10 40.78 42.23 12.62  4.85   \\n\",\n",
    "       \"2                    0.20  79   88   25  15 38.35 42.72 12.14  7.28   \\n\",\n",
    "       \"3                    0.30  78   89   24  16 37.86 43.20 11.65  7.77   \\n\",\n",
    "       \"4                    0.40  78   92   21  16 37.86 44.66 10.19  7.77   \\n\",\n",
    "       \"5                    0.50  76  100   13  18 36.89 48.54  6.31  8.74   \\n\",\n",
    "       \"6                    0.60  76  100   13  18 36.89 48.54  6.31  8.74   \\n\",\n",
    "       \"7                    0.70  70  102   11  24 33.98 49.51  5.34 11.65   \\n\",\n",
    "       \"8                    0.80  69  102   11  25 33.50 49.51  5.34 12.14   \\n\",\n",
    "       \"9                    0.90  69  107    6  25 33.50 51.94  2.91 12.14   \\n\",\n",
    "       \"10                   1.00   6  113    0  88  2.91 54.85  0.00 42.72   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Precision  Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0        0.45    1.00      0.45 0.62 0.50  \\n\",\n",
    "       \"1        0.76    0.89      0.83 0.82 0.83  \\n\",\n",
    "       \"2        0.76    0.84      0.81 0.80 0.81  \\n\",\n",
    "       \"3        0.77    0.83      0.81 0.80 0.81  \\n\",\n",
    "       \"4        0.79    0.83      0.82 0.81 0.82  \\n\",\n",
    "       \"5        0.85    0.81      0.85 0.83 0.85  \\n\",\n",
    "       \"6        0.85    0.81      0.85 0.83 0.85  \\n\",\n",
    "       \"7        0.86    0.74      0.83 0.80 0.82  \\n\",\n",
    "       \"8        0.86    0.73      0.83 0.79 0.82  \\n\",\n",
    "       \"9        0.92    0.73      0.85 0.82 0.84  \\n\",\n",
    "       \"10       1.00    0.06      0.57 0.12 0.53  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 40,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"best_decision_tree = DecisionTreeClassifier(max_depth=30,min_impurity_decrease=0.001,min_samples_split=15,random_state=1)\\n\",\n",
    "    \"best_decision_tree.fit(X_train,y_train)\\n\",\n",
    "    \"class_perf_measures(best_decision_tree,X_test,y_test,0,1,0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 41,\n",
    "   \"id\": \"40a6b9e1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>0.62</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.10</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>65</td>\\n\",\n",
    "       \"      <td>48</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>31.55</td>\\n\",\n",
    "       \"      <td>23.30</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.64</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>79</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>38.35</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>7.28</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>95</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>46.12</td>\\n\",\n",
    "       \"      <td>8.74</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.60</td>\\n\",\n",
    "       \"      <td>77</td>\\n\",\n",
    "       \"      <td>104</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>37.38</td>\\n\",\n",
    "       \"      <td>50.49</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>8.25</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>68</td>\\n\",\n",
    "       \"      <td>107</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>26</td>\\n\",\n",
    "       \"      <td>33.01</td>\\n\",\n",
    "       \"      <td>51.94</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>12.62</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.72</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>67</td>\\n\",\n",
    "       \"      <td>107</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>32.52</td>\\n\",\n",
    "       \"      <td>51.94</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.71</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>65</td>\\n\",\n",
    "       \"      <td>107</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>29</td>\\n\",\n",
    "       \"      <td>31.55</td>\\n\",\n",
    "       \"      <td>51.94</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>14.08</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.69</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>47</td>\\n\",\n",
    "       \"      <td>110</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>47</td>\\n\",\n",
    "       \"      <td>22.82</td>\\n\",\n",
    "       \"      <td>53.40</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>22.82</td>\\n\",\n",
    "       \"      <td>0.94</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.65</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN   FP  FN   TP%   TN%   FP%   FN%  \\\\\\n\",\n",
    "       \"0                    0.00  94    0  113   0 45.63  0.00 54.85  0.00   \\n\",\n",
    "       \"1                    0.10  87   65   48   7 42.23 31.55 23.30  3.40   \\n\",\n",
    "       \"2                    0.20  79   90   23  15 38.35 43.69 11.17  7.28   \\n\",\n",
    "       \"3                    0.30  78   91   22  16 37.86 44.17 10.68  7.77   \\n\",\n",
    "       \"4                    0.40  78   91   22  16 37.86 44.17 10.68  7.77   \\n\",\n",
    "       \"5                    0.50  78   95   18  16 37.86 46.12  8.74  7.77   \\n\",\n",
    "       \"6                    0.60  77  104    9  17 37.38 50.49  4.37  8.25   \\n\",\n",
    "       \"7                    0.70  68  107    6  26 33.01 51.94  2.91 12.62   \\n\",\n",
    "       \"8                    0.80  67  107    6  27 32.52 51.94  2.91 13.11   \\n\",\n",
    "       \"9                    0.90  65  107    6  29 31.55 51.94  2.91 14.08   \\n\",\n",
    "       \"10                   1.00  47  110    3  47 22.82 53.40  1.46 22.82   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Precision  Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0        0.45    1.00      0.45 0.62 0.50  \\n\",\n",
    "       \"1        0.64    0.93      0.73 0.76 0.75  \\n\",\n",
    "       \"2        0.78    0.84      0.82 0.81 0.82  \\n\",\n",
    "       \"3        0.78    0.83      0.82 0.80 0.82  \\n\",\n",
    "       \"4        0.78    0.83      0.82 0.80 0.82  \\n\",\n",
    "       \"5        0.81    0.83      0.84 0.82 0.83  \\n\",\n",
    "       \"6        0.90    0.82      0.87 0.86 0.87  \\n\",\n",
    "       \"7        0.92    0.72      0.85 0.81 0.83  \\n\",\n",
    "       \"8        0.92    0.71      0.84 0.80 0.83  \\n\",\n",
    "       \"9        0.92    0.69      0.83 0.79 0.82  \\n\",\n",
    "       \"10       0.94    0.50      0.76 0.65 0.74  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 41,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"bestClassTree = gridSearch.best_estimator_\\n\",\n",
    "    \"class_perf_measures(bestClassTree,X_test,y_test,0,1,0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"8ff8b097\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"- Here I am choosing the probability threshold of 0.6 by considering the AUC, Accuracy and Precision values\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 42,\n",
    "   \"id\": \"e1a93fd4\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"decision_tree_threshold = 0.6\\n\",\n",
    "    \"\\n\",\n",
    "    \"decision_tree_pred = (bestClassTree.predict_proba(X_test)[:,1] >= decision_tree_threshold).astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_used = (gridSearch.best_estimator_.feature_importances_>0).sum()\\n\",\n",
    "    \"hyperparameters = bestClassTree.get_params()\\n\",\n",
    "    \"model = \\\"Decision tree\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,decision_tree_pred,features_used,total_time,df_results,hyperparameters)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 43,\n",
    "   \"id\": \"2359d75c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Model</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"      <th>Total time taken</th>\\n\",\n",
    "       \"      <th>False Negative</th>\\n\",\n",
    "       \"      <th>False Positive</th>\\n\",\n",
    "       \"      <th>Features Used</th>\\n\",\n",
    "       \"      <th>Hyperparameters</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Logistic Regression</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>Logistic Regression for Selected Features</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>kNN Classification</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>Decision tree</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"                                       Model  Accuracy  Recall  Precision  \\\\\\n\",\n",
    "       \"0                        Logistic Regression      0.88    0.86       0.88   \\n\",\n",
    "       \"1  Logistic Regression for Selected Features      0.88    0.86       0.88   \\n\",\n",
    "       \"2                         kNN Classification      0.81    0.73       0.82   \\n\",\n",
    "       \"3                              Decision tree      0.87    0.82       0.90   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   AUC  Total time taken  False Negative  False Positive  Features Used  \\\\\\n\",\n",
    "       \"0 0.88              0.03              13              11             14   \\n\",\n",
    "       \"1 0.88              0.00              13              11             14   \\n\",\n",
    "       \"2 0.80              0.82              25              15             14   \\n\",\n",
    "       \"3 0.87              0.82              17               9             12   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                                                            Hyperparameters  \\n\",\n",
    "       \"0                                                                                       nan  \\n\",\n",
    "       \"1                                                                                       nan  \\n\",\n",
    "       \"2                                                                                       nan  \\n\",\n",
    "       \"3  {'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 43,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"b6ec1185\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"- We did the pruning using max depth, minimum impurity decrease and minimum samples split. Now, we want to use the cost complexity pruning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 44,\n",
    "   \"id\": \"be5747a5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"21\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 44,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"path = bestClassTree.cost_complexity_pruning_path(X_train, y_train)\\n\",\n",
    "    \"ccp_alphas = path.ccp_alphas\\n\",\n",
    "    \"ccp_alphas = ccp_alphas[:-1]\\n\",\n",
    "    \"len(ccp_alphas)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 45,\n",
    "   \"id\": \"24632564\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"image/png\": \"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\\n\",\n",
    "      \"text/plain\": [\n",
    "       \"<Figure size 432x288 with 1 Axes>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {\n",
    "      \"needs_background\": \"light\"\n",
    "     },\n",
    "     \"output_type\": \"display_data\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"dts = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for ccp_alpha in ccp_alphas:\\n\",\n",
    "    \"    dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\\n\",\n",
    "    \"    dt.fit(X_train,y_train)\\n\",\n",
    "    \"    dts.append(dt)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"train_scores = [dt.score(X_train,y_train) for dt in dts]\\n\",\n",
    "    \"test_scores = [dt.score(X_test,y_test) for dt in dts]\\n\",\n",
    "    \"\\n\",\n",
    "    \"fix, ax = plt.subplots()\\n\",\n",
    "    \"ax.set_xlabel('alpha')\\n\",\n",
    "    \"ax.set_ylabel('accuracy')\\n\",\n",
    "    \"ax.set_title(\\\"Accuracy vs Alpha for Training and Testing Sets\\\")\\n\",\n",
    "    \"ax.plot(ccp_alphas, train_scores, marker='o', label=\\\"train\\\", drawstyle=\\\"steps-post\\\")\\n\",\n",
    "    \"ax.plot(ccp_alphas, test_scores, marker='o', label=\\\"test\\\",drawstyle='steps-post')\\n\",\n",
    "    \"ax.legend()\\n\",\n",
    "    \"plt.show()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 46,\n",
    "   \"id\": \"d132e9d4\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"image/png\": \"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\\n\",\n",
    "      \"text/plain\": [\n",
    "       \"<Figure size 432x288 with 1 Axes>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {\n",
    "      \"needs_background\": \"light\"\n",
    "     },\n",
    "     \"output_type\": \"display_data\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"fix, ax = plt.subplots()\\n\",\n",
    "    \"ax.set_xlabel('alpha')\\n\",\n",
    "    \"ax.set_ylabel('accuracy')\\n\",\n",
    "    \"ax.set_title(\\\"Accuracy vs Alpha for Training and Testing Sets\\\")\\n\",\n",
    "    \"ax.plot(ccp_alphas[0::2], train_scores[0::2], marker='o', label=\\\"train\\\", drawstyle=\\\"steps-post\\\")\\n\",\n",
    "    \"ax.plot(ccp_alphas[0::2], test_scores[0::2], marker='o', label=\\\"test\\\",drawstyle='steps-post')\\n\",\n",
    "    \"ax.legend()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 47,\n",
    "   \"id\": \"79952c08\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"array([0.        , 0.00017141, 0.00020661, 0.0002933 , 0.0003545 ,\\n\",\n",
    "       \"       0.0004083 , 0.00044366, 0.00069876, 0.00166617, 0.00204327,\\n\",\n",
    "       \"       0.00270798, 0.00325761, 0.00383719, 0.00393893, 0.00419204,\\n\",\n",
    "       \"       0.00463954, 0.00490759, 0.00522251, 0.01009734, 0.01486442,\\n\",\n",
    "       \"       0.0379385 ])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 47,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"ccp_alphas\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 48,\n",
    "   \"id\": \"6abb9de0\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>0.62</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.10</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>71</td>\\n\",\n",
    "       \"      <td>104</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>34.47</td>\\n\",\n",
    "       \"      <td>50.49</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.60</td>\\n\",\n",
    "       \"      <td>71</td>\\n\",\n",
    "       \"      <td>104</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>34.47</td>\\n\",\n",
    "       \"      <td>50.49</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>71</td>\\n\",\n",
    "       \"      <td>104</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>34.47</td>\\n\",\n",
    "       \"      <td>50.49</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>71</td>\\n\",\n",
    "       \"      <td>104</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>34.47</td>\\n\",\n",
    "       \"      <td>50.49</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>71</td>\\n\",\n",
    "       \"      <td>104</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>34.47</td>\\n\",\n",
    "       \"      <td>50.49</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>93</td>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>45.15</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.01</td>\\n\",\n",
    "       \"      <td>0.55</td>\\n\",\n",
    "       \"      <td>0.02</td>\\n\",\n",
    "       \"      <td>0.51</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN   FP  FN   TP%   TN%   FP%   FN%  \\\\\\n\",\n",
    "       \"0                    0.00  94    0  113   0 45.63  0.00 54.85  0.00   \\n\",\n",
    "       \"1                    0.10  86   86   27   8 41.75 41.75 13.11  3.88   \\n\",\n",
    "       \"2                    0.20  86   86   27   8 41.75 41.75 13.11  3.88   \\n\",\n",
    "       \"3                    0.30  86   86   27   8 41.75 41.75 13.11  3.88   \\n\",\n",
    "       \"4                    0.40  86   86   27   8 41.75 41.75 13.11  3.88   \\n\",\n",
    "       \"5                    0.50  71  104    9  23 34.47 50.49  4.37 11.17   \\n\",\n",
    "       \"6                    0.60  71  104    9  23 34.47 50.49  4.37 11.17   \\n\",\n",
    "       \"7                    0.70  71  104    9  23 34.47 50.49  4.37 11.17   \\n\",\n",
    "       \"8                    0.80  71  104    9  23 34.47 50.49  4.37 11.17   \\n\",\n",
    "       \"9                    0.90  71  104    9  23 34.47 50.49  4.37 11.17   \\n\",\n",
    "       \"10                   1.00   1  113    0  93  0.49 54.85  0.00 45.15   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Precision  Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0        0.45    1.00      0.45 0.62 0.50  \\n\",\n",
    "       \"1        0.76    0.92      0.83 0.83 0.84  \\n\",\n",
    "       \"2        0.76    0.92      0.83 0.83 0.84  \\n\",\n",
    "       \"3        0.76    0.92      0.83 0.83 0.84  \\n\",\n",
    "       \"4        0.76    0.92      0.83 0.83 0.84  \\n\",\n",
    "       \"5        0.89    0.76      0.85 0.82 0.84  \\n\",\n",
    "       \"6        0.89    0.76      0.85 0.82 0.84  \\n\",\n",
    "       \"7        0.89    0.76      0.85 0.82 0.84  \\n\",\n",
    "       \"8        0.89    0.76      0.85 0.82 0.84  \\n\",\n",
    "       \"9        0.89    0.76      0.85 0.82 0.84  \\n\",\n",
    "       \"10       1.00    0.01      0.55 0.02 0.51  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 48,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"decision_tree_ccp = DecisionTreeClassifier(random_state=1, ccp_alpha=0.00522251)\\n\",\n",
    "    \"decision_tree_ccp.fit(X_train,y_train)\\n\",\n",
    "    \"total_time = time.time() - start_time\\n\",\n",
    "    \"class_perf_measures(decision_tree_ccp,X_test,y_test,0,1,0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"fac13030\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"- Here, we shall choose probability threshold of 0.50 by considering AUC, Accuracy and Precision\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 49,\n",
    "   \"id\": \"c45b96e7\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"decision_tree_ccp_threshold = 0.5\\n\",\n",
    "    \"\\n\",\n",
    "    \"decision_tree_ccp_pred = (decision_tree_ccp.predict_proba(X_test)[:,1] >= decision_tree_ccp_threshold).astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_used = (decision_tree_ccp.feature_importances_>0).sum()\\n\",\n",
    "    \"hyperparameters = decision_tree_ccp.get_params()\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = \\\"Decision Tree Cost Complexity Pruning\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,decision_tree_ccp_pred,features_used,total_time,df_results,hyperparameters)\\n\",\n",
    "    \"               \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 50,\n",
    "   \"id\": \"6bbfbb35\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Model</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"      <th>Total time taken</th>\\n\",\n",
    "       \"      <th>False Negative</th>\\n\",\n",
    "       \"      <th>False Positive</th>\\n\",\n",
    "       \"      <th>Features Used</th>\\n\",\n",
    "       \"      <th>Hyperparameters</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Logistic Regression</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>Logistic Regression for Selected Features</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>kNN Classification</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>Decision tree</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>Decision Tree Cost Complexity Pruning</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"                                       Model  Accuracy  Recall  Precision  \\\\\\n\",\n",
    "       \"0                        Logistic Regression      0.88    0.86       0.88   \\n\",\n",
    "       \"1  Logistic Regression for Selected Features      0.88    0.86       0.88   \\n\",\n",
    "       \"2                         kNN Classification      0.81    0.73       0.82   \\n\",\n",
    "       \"3                              Decision tree      0.87    0.82       0.90   \\n\",\n",
    "       \"4      Decision Tree Cost Complexity Pruning      0.84    0.76       0.89   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   AUC  Total time taken  False Negative  False Positive  Features Used  \\\\\\n\",\n",
    "       \"0 0.88              0.03              13              11             14   \\n\",\n",
    "       \"1 0.88              0.00              13              11             14   \\n\",\n",
    "       \"2 0.80              0.82              25              15             14   \\n\",\n",
    "       \"3 0.87              0.82              17               9             12   \\n\",\n",
    "       \"4 0.84              0.00              23               9              5   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                                                                                                                                                                                                                                                              Hyperparameters  \\n\",\n",
    "       \"0                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"1                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"2                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"3                                                                                                                                                                                                    {'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}  \\n\",\n",
    "       \"4  {'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 50,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 51,\n",
    "   \"id\": \"0341e4e1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# decision_tree_ccp = DecisionTreeClassifier(random_state=1, ccp_alpha=0.00522251)\\n\",\n",
    "    \"# decision_tree_ccp.fit(X_train,y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# plt.figure(dpi=800)\\n\",\n",
    "    \"# fig = plt.figure(figsize=(150,75))\\n\",\n",
    "    \"# plot_tree(decision_tree_ccp,\\n\",\n",
    "    \"#           filled=True,\\n\",\n",
    "    \"#           rounded=True,\\n\",\n",
    "    \"#           class_names=['Not Approved','Approved'],\\n\",\n",
    "    \"#           feature_names=X_train.columns\\n\",\n",
    "    \"#          );\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"5d13b078\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.4 Random Forest<a class=\\\"anchor\\\" id=\\\"fourth-model\\\"></a>\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"*  [Go to ML Models](#maclrn)\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\\n\",\n",
    "    \"\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"Random Forest is a tree-based machine learning algorithm that leverages the power of multiple decision trees for making decisions. As the name suggests, it is a “forest” of trees!\\n\",\n",
    "    \"\\n\",\n",
    "    \"But why do we call it a “random” forest? That’s because it is a forest of randomly created decision trees. Each node in the decision tree works on a random subset of features to calculate the output. The random forest then combines the output of individual decision trees to generate the final output.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The Random Forest Algorithm combines the output of multiple (randomly created) Decision Trees to generate the final output. This process of combining the output of multiple individual models (also known as weak learners) is called Ensemble Learning. \\n\",\n",
    "    \"\\n\",\n",
    "    \"For more details: https://www.analyticsvidhya.com/blog/2020/05/decision-tree-vs-random-forest-algorithm/\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src = 'https://miro.medium.com/max/1200/1*hmtbIgxoflflJqMJ_UHwXw.jpeg'/>\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 52,\n",
    "   \"id\": \"aac94ebd\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  self.best_estimator_.fit(X, y, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\\n\",\n",
    "      \"  _warn_prf(average, modifier, msg_start, len(result))\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>0.62</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.10</td>\\n\",\n",
    "       \"      <td>93</td>\\n\",\n",
    "       \"      <td>37</td>\\n\",\n",
    "       \"      <td>76</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>45.15</td>\\n\",\n",
    "       \"      <td>17.96</td>\\n\",\n",
    "       \"      <td>36.89</td>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>0.55</td>\\n\",\n",
    "       \"      <td>0.99</td>\\n\",\n",
    "       \"      <td>0.63</td>\\n\",\n",
    "       \"      <td>0.71</td>\\n\",\n",
    "       \"      <td>0.66</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>35</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>16.99</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.72</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.60</td>\\n\",\n",
    "       \"      <td>72</td>\\n\",\n",
    "       \"      <td>105</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>34.95</td>\\n\",\n",
    "       \"      <td>50.97</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>62</td>\\n\",\n",
    "       \"      <td>107</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>32</td>\\n\",\n",
    "       \"      <td>30.10</td>\\n\",\n",
    "       \"      <td>51.94</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>15.53</td>\\n\",\n",
    "       \"      <td>0.91</td>\\n\",\n",
    "       \"      <td>0.66</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>48</td>\\n\",\n",
    "       \"      <td>109</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>46</td>\\n\",\n",
    "       \"      <td>23.30</td>\\n\",\n",
    "       \"      <td>52.91</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>22.33</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.51</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.66</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>112</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>67</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>54.37</td>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>32.52</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.29</td>\\n\",\n",
    "       \"      <td>0.67</td>\\n\",\n",
    "       \"      <td>0.44</td>\\n\",\n",
    "       \"      <td>0.64</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.55</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN   FP  FN   TP%   TN%   FP%   FN%  \\\\\\n\",\n",
    "       \"0                    0.00  94    0  113   0 45.63  0.00 54.85  0.00   \\n\",\n",
    "       \"1                    0.10  93   37   76   1 45.15 17.96 36.89  0.49   \\n\",\n",
    "       \"2                    0.20  92   78   35   2 44.66 37.86 16.99  0.97   \\n\",\n",
    "       \"3                    0.30  91   90   23   3 44.17 43.69 11.17  1.46   \\n\",\n",
    "       \"4                    0.40  84   99   14  10 40.78 48.06  6.80  4.85   \\n\",\n",
    "       \"5                    0.50  78  102   11  16 37.86 49.51  5.34  7.77   \\n\",\n",
    "       \"6                    0.60  72  105    8  22 34.95 50.97  3.88 10.68   \\n\",\n",
    "       \"7                    0.70  62  107    6  32 30.10 51.94  2.91 15.53   \\n\",\n",
    "       \"8                    0.80  48  109    4  46 23.30 52.91  1.94 22.33   \\n\",\n",
    "       \"9                    0.90  27  112    1  67 13.11 54.37  0.49 32.52   \\n\",\n",
    "       \"10                   1.00   0  113    0  94  0.00 54.85  0.00 45.63   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Precision  Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0        0.45    1.00      0.45 0.62 0.50  \\n\",\n",
    "       \"1        0.55    0.99      0.63 0.71 0.66  \\n\",\n",
    "       \"2        0.72    0.98      0.82 0.83 0.83  \\n\",\n",
    "       \"3        0.80    0.97      0.87 0.88 0.88  \\n\",\n",
    "       \"4        0.86    0.89      0.88 0.88 0.89  \\n\",\n",
    "       \"5        0.88    0.83      0.87 0.85 0.87  \\n\",\n",
    "       \"6        0.90    0.77      0.86 0.83 0.85  \\n\",\n",
    "       \"7        0.91    0.66      0.82 0.77 0.80  \\n\",\n",
    "       \"8        0.92    0.51      0.76 0.66 0.74  \\n\",\n",
    "       \"9        0.96    0.29      0.67 0.44 0.64  \\n\",\n",
    "       \"10       0.00    0.00      0.55 0.00 0.50  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 52,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"random_forest_2 = RandomForestClassifier(random_state=1)\\n\",\n",
    "    \"param_grid = { \\n\",\n",
    "    \"    'max_depth': [15,20],\\n\",\n",
    "    \"    'min_samples_split': [20],\\n\",\n",
    "    \"    'min_impurity_decrease': [0, 0.0001],\\n\",\n",
    "    \"    'criterion' :['gini', 'entropy'],\\n\",\n",
    "    \"    'ccp_alpha' : [ 0.00010131, 0.00010153]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"random_forest_grid_search_2 = GridSearchCV(estimator=random_forest_2, param_grid=param_grid, cv= 5,scoring=['accuracy','recall','precision','roc_auc'], refit='roc_auc')\\n\",\n",
    "    \"random_forest_grid_search_2.fit(X_train, y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_random_forest_2 = random_forest_grid_search_2.best_estimator_\\n\",\n",
    "    \"total_time = time.time() - start_time\\n\",\n",
    "    \"\\n\",\n",
    "    \"class_perf_measures(best_random_forest_2,X_test,y_test,0,1,0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 53,\n",
    "   \"id\": \"82a468fd\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.31</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.32</td>\\n\",\n",
    "       \"      <td>89</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>43.20</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>2.43</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.95</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.33</td>\\n\",\n",
    "       \"      <td>88</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>42.72</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.94</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.34</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>10.19</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.35</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>10.19</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.36</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>10.19</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.37</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>95</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>46.12</td>\\n\",\n",
    "       \"      <td>8.74</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.38</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>97</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>47.09</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.39</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>11</th>\\n\",\n",
    "       \"      <td>0.41</td>\\n\",\n",
    "       \"      <td>83</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>40.29</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP  TN  FP  FN   TP%   TN%   FP%  FN%  Precision  \\\\\\n\",\n",
    "       \"0                    0.30  91  90  23   3 44.17 43.69 11.17 1.46       0.80   \\n\",\n",
    "       \"1                    0.31  91  91  22   3 44.17 44.17 10.68 1.46       0.81   \\n\",\n",
    "       \"2                    0.32  89  91  22   5 43.20 44.17 10.68 2.43       0.80   \\n\",\n",
    "       \"3                    0.33  88  91  22   6 42.72 44.17 10.68 2.91       0.80   \\n\",\n",
    "       \"4                    0.34  87  92  21   7 42.23 44.66 10.19 3.40       0.81   \\n\",\n",
    "       \"5                    0.35  87  92  21   7 42.23 44.66 10.19 3.40       0.81   \\n\",\n",
    "       \"6                    0.36  87  92  21   7 42.23 44.66 10.19 3.40       0.81   \\n\",\n",
    "       \"7                    0.37  85  95  18   9 41.26 46.12  8.74 4.37       0.82   \\n\",\n",
    "       \"8                    0.38  85  97  16   9 41.26 47.09  7.77 4.37       0.84   \\n\",\n",
    "       \"9                    0.39  84  99  14  10 40.78 48.06  6.80 4.85       0.86   \\n\",\n",
    "       \"10                   0.40  84  99  14  10 40.78 48.06  6.80 4.85       0.86   \\n\",\n",
    "       \"11                   0.41  83  99  14  11 40.29 48.06  6.80 5.34       0.86   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0     0.97      0.87 0.88 0.88  \\n\",\n",
    "       \"1     0.97      0.88 0.88 0.89  \\n\",\n",
    "       \"2     0.95      0.87 0.87 0.88  \\n\",\n",
    "       \"3     0.94      0.86 0.86 0.87  \\n\",\n",
    "       \"4     0.93      0.86 0.86 0.87  \\n\",\n",
    "       \"5     0.93      0.86 0.86 0.87  \\n\",\n",
    "       \"6     0.93      0.86 0.86 0.87  \\n\",\n",
    "       \"7     0.90      0.87 0.86 0.87  \\n\",\n",
    "       \"8     0.90      0.88 0.87 0.88  \\n\",\n",
    "       \"9     0.89      0.88 0.88 0.89  \\n\",\n",
    "       \"10    0.89      0.88 0.88 0.89  \\n\",\n",
    "       \"11    0.88      0.88 0.87 0.88  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 53,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"class_perf_measures(best_random_forest_2,X_test,y_test,0.3,0.4,0.01)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 54,\n",
    "   \"id\": \"628e89c4\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"random_forest_threshold_2 = 0.31\\n\",\n",
    "    \"\\n\",\n",
    "    \"random_forest_pred_2 = (best_random_forest_2.predict_proba(X_test)[:,1] >= random_forest_threshold_2).astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_used = (best_random_forest_2.feature_importances_>0).sum()\\n\",\n",
    "    \"hyperparameters = best_random_forest_2.get_params()\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = \\\"Random Forest\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,random_forest_pred_2,features_used,total_time,df_results,hyperparameters)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 55,\n",
    "   \"id\": \"26725ef0\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Model</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"      <th>Total time taken</th>\\n\",\n",
    "       \"      <th>False Negative</th>\\n\",\n",
    "       \"      <th>False Positive</th>\\n\",\n",
    "       \"      <th>Features Used</th>\\n\",\n",
    "       \"      <th>Hyperparameters</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Logistic Regression</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>Logistic Regression for Selected Features</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>kNN Classification</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>Decision tree</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>Decision Tree Cost Complexity Pruning</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>Random Forest</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>14.54</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"                                       Model  Accuracy  Recall  Precision  \\\\\\n\",\n",
    "       \"0                        Logistic Regression      0.88    0.86       0.88   \\n\",\n",
    "       \"1  Logistic Regression for Selected Features      0.88    0.86       0.88   \\n\",\n",
    "       \"2                         kNN Classification      0.81    0.73       0.82   \\n\",\n",
    "       \"3                              Decision tree      0.87    0.82       0.90   \\n\",\n",
    "       \"4      Decision Tree Cost Complexity Pruning      0.84    0.76       0.89   \\n\",\n",
    "       \"5                              Random Forest      0.88    0.97       0.81   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   AUC  Total time taken  False Negative  False Positive  Features Used  \\\\\\n\",\n",
    "       \"0 0.88              0.03              13              11             14   \\n\",\n",
    "       \"1 0.88              0.00              13              11             14   \\n\",\n",
    "       \"2 0.80              0.82              25              15             14   \\n\",\n",
    "       \"3 0.87              0.82              17               9             12   \\n\",\n",
    "       \"4 0.84              0.00              23               9              5   \\n\",\n",
    "       \"5 0.89             14.54               3              22             15   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                                                                                                                                                                                                                                                              Hyperparameters  \\n\",\n",
    "       \"0                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"1                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"2                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"3                                                                                                                                                                                                    {'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}  \\n\",\n",
    "       \"4  {'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}  \\n\",\n",
    "       \"5                                                                                                                                                                                        {'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 55,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"bb883c48\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Using selected features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 56,\n",
    "   \"id\": \"945e92db\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  self.best_estimator_.fit(X, y, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\\n\",\n",
    "      \"  _warn_prf(average, modifier, msg_start, len(result))\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>0.62</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.10</td>\\n\",\n",
    "       \"      <td>93</td>\\n\",\n",
    "       \"      <td>37</td>\\n\",\n",
    "       \"      <td>76</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>45.15</td>\\n\",\n",
    "       \"      <td>17.96</td>\\n\",\n",
    "       \"      <td>36.89</td>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>0.55</td>\\n\",\n",
    "       \"      <td>0.99</td>\\n\",\n",
    "       \"      <td>0.63</td>\\n\",\n",
    "       \"      <td>0.71</td>\\n\",\n",
    "       \"      <td>0.66</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>35</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>16.99</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.72</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.60</td>\\n\",\n",
    "       \"      <td>72</td>\\n\",\n",
    "       \"      <td>105</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>34.95</td>\\n\",\n",
    "       \"      <td>50.97</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>62</td>\\n\",\n",
    "       \"      <td>107</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>32</td>\\n\",\n",
    "       \"      <td>30.10</td>\\n\",\n",
    "       \"      <td>51.94</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>15.53</td>\\n\",\n",
    "       \"      <td>0.91</td>\\n\",\n",
    "       \"      <td>0.66</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>48</td>\\n\",\n",
    "       \"      <td>109</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>46</td>\\n\",\n",
    "       \"      <td>23.30</td>\\n\",\n",
    "       \"      <td>52.91</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>22.33</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.51</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.66</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>112</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>67</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>54.37</td>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>32.52</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.29</td>\\n\",\n",
    "       \"      <td>0.67</td>\\n\",\n",
    "       \"      <td>0.44</td>\\n\",\n",
    "       \"      <td>0.64</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.55</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN   FP  FN   TP%   TN%   FP%   FN%  \\\\\\n\",\n",
    "       \"0                    0.00  94    0  113   0 45.63  0.00 54.85  0.00   \\n\",\n",
    "       \"1                    0.10  93   37   76   1 45.15 17.96 36.89  0.49   \\n\",\n",
    "       \"2                    0.20  92   78   35   2 44.66 37.86 16.99  0.97   \\n\",\n",
    "       \"3                    0.30  91   90   23   3 44.17 43.69 11.17  1.46   \\n\",\n",
    "       \"4                    0.40  84   99   14  10 40.78 48.06  6.80  4.85   \\n\",\n",
    "       \"5                    0.50  78  102   11  16 37.86 49.51  5.34  7.77   \\n\",\n",
    "       \"6                    0.60  72  105    8  22 34.95 50.97  3.88 10.68   \\n\",\n",
    "       \"7                    0.70  62  107    6  32 30.10 51.94  2.91 15.53   \\n\",\n",
    "       \"8                    0.80  48  109    4  46 23.30 52.91  1.94 22.33   \\n\",\n",
    "       \"9                    0.90  27  112    1  67 13.11 54.37  0.49 32.52   \\n\",\n",
    "       \"10                   1.00   0  113    0  94  0.00 54.85  0.00 45.63   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Precision  Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0        0.45    1.00      0.45 0.62 0.50  \\n\",\n",
    "       \"1        0.55    0.99      0.63 0.71 0.66  \\n\",\n",
    "       \"2        0.72    0.98      0.82 0.83 0.83  \\n\",\n",
    "       \"3        0.80    0.97      0.87 0.88 0.88  \\n\",\n",
    "       \"4        0.86    0.89      0.88 0.88 0.89  \\n\",\n",
    "       \"5        0.88    0.83      0.87 0.85 0.87  \\n\",\n",
    "       \"6        0.90    0.77      0.86 0.83 0.85  \\n\",\n",
    "       \"7        0.91    0.66      0.82 0.77 0.80  \\n\",\n",
    "       \"8        0.92    0.51      0.76 0.66 0.74  \\n\",\n",
    "       \"9        0.96    0.29      0.67 0.44 0.64  \\n\",\n",
    "       \"10       0.00    0.00      0.55 0.00 0.50  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 56,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"random_forest_3 = RandomForestClassifier(random_state=1)\\n\",\n",
    "    \"param_grid = { \\n\",\n",
    "    \"    'max_depth': [15,20],\\n\",\n",
    "    \"    'min_samples_split': [20],\\n\",\n",
    "    \"    'min_impurity_decrease': [0, 0.0001],\\n\",\n",
    "    \"    'criterion' :['gini', 'entropy'],\\n\",\n",
    "    \"    'ccp_alpha' : [ 0.00010131, 0.00010153]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"random_forest_grid_search_3 = GridSearchCV(estimator=random_forest_3, param_grid=param_grid, cv= 5,scoring=['accuracy','recall','precision','roc_auc'],\\n\",\n",
    "    \"                          refit='roc_auc')\\n\",\n",
    "    \"random_forest_grid_search_3.fit(X_train[predictors], y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_random_forest_3 = random_forest_grid_search_3.best_estimator_\\n\",\n",
    "    \"total_time = time.time() - start_time\\n\",\n",
    "    \"\\n\",\n",
    "    \"class_perf_measures(best_random_forest_3,X_test[predictors],y_test,0,1,0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 57,\n",
    "   \"id\": \"5396388c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.31</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.32</td>\\n\",\n",
    "       \"      <td>89</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>43.20</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>2.43</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.95</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.33</td>\\n\",\n",
    "       \"      <td>88</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>42.72</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.94</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.34</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>10.19</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.35</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>10.19</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.36</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>10.19</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.37</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>95</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>46.12</td>\\n\",\n",
    "       \"      <td>8.74</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.38</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>97</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>47.09</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.39</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>11</th>\\n\",\n",
    "       \"      <td>0.41</td>\\n\",\n",
    "       \"      <td>83</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>40.29</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>12</th>\\n\",\n",
    "       \"      <td>0.42</td>\\n\",\n",
    "       \"      <td>83</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>40.29</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>13</th>\\n\",\n",
    "       \"      <td>0.43</td>\\n\",\n",
    "       \"      <td>83</td>\\n\",\n",
    "       \"      <td>100</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>40.29</td>\\n\",\n",
    "       \"      <td>48.54</td>\\n\",\n",
    "       \"      <td>6.31</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>14</th>\\n\",\n",
    "       \"      <td>0.44</td>\\n\",\n",
    "       \"      <td>81</td>\\n\",\n",
    "       \"      <td>101</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>39.32</td>\\n\",\n",
    "       \"      <td>49.03</td>\\n\",\n",
    "       \"      <td>5.83</td>\\n\",\n",
    "       \"      <td>6.31</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>15</th>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>80</td>\\n\",\n",
    "       \"      <td>101</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>38.83</td>\\n\",\n",
    "       \"      <td>49.03</td>\\n\",\n",
    "       \"      <td>5.83</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>16</th>\\n\",\n",
    "       \"      <td>0.46</td>\\n\",\n",
    "       \"      <td>79</td>\\n\",\n",
    "       \"      <td>101</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>38.35</td>\\n\",\n",
    "       \"      <td>49.03</td>\\n\",\n",
    "       \"      <td>5.83</td>\\n\",\n",
    "       \"      <td>7.28</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>17</th>\\n\",\n",
    "       \"      <td>0.47</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>18</th>\\n\",\n",
    "       \"      <td>0.48</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>19</th>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>20</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN  FP  FN   TP%   TN%   FP%  FN%  Precision  \\\\\\n\",\n",
    "       \"0                    0.30  91   90  23   3 44.17 43.69 11.17 1.46       0.80   \\n\",\n",
    "       \"1                    0.31  91   91  22   3 44.17 44.17 10.68 1.46       0.81   \\n\",\n",
    "       \"2                    0.32  89   91  22   5 43.20 44.17 10.68 2.43       0.80   \\n\",\n",
    "       \"3                    0.33  88   91  22   6 42.72 44.17 10.68 2.91       0.80   \\n\",\n",
    "       \"4                    0.34  87   92  21   7 42.23 44.66 10.19 3.40       0.81   \\n\",\n",
    "       \"5                    0.35  87   92  21   7 42.23 44.66 10.19 3.40       0.81   \\n\",\n",
    "       \"6                    0.36  87   92  21   7 42.23 44.66 10.19 3.40       0.81   \\n\",\n",
    "       \"7                    0.37  85   95  18   9 41.26 46.12  8.74 4.37       0.82   \\n\",\n",
    "       \"8                    0.38  85   97  16   9 41.26 47.09  7.77 4.37       0.84   \\n\",\n",
    "       \"9                    0.39  84   99  14  10 40.78 48.06  6.80 4.85       0.86   \\n\",\n",
    "       \"10                   0.40  84   99  14  10 40.78 48.06  6.80 4.85       0.86   \\n\",\n",
    "       \"11                   0.41  83   99  14  11 40.29 48.06  6.80 5.34       0.86   \\n\",\n",
    "       \"12                   0.42  83   99  14  11 40.29 48.06  6.80 5.34       0.86   \\n\",\n",
    "       \"13                   0.43  83  100  13  11 40.29 48.54  6.31 5.34       0.86   \\n\",\n",
    "       \"14                   0.44  81  101  12  13 39.32 49.03  5.83 6.31       0.87   \\n\",\n",
    "       \"15                   0.45  80  101  12  14 38.83 49.03  5.83 6.80       0.87   \\n\",\n",
    "       \"16                   0.46  79  101  12  15 38.35 49.03  5.83 7.28       0.87   \\n\",\n",
    "       \"17                   0.47  78  102  11  16 37.86 49.51  5.34 7.77       0.88   \\n\",\n",
    "       \"18                   0.48  78  102  11  16 37.86 49.51  5.34 7.77       0.88   \\n\",\n",
    "       \"19                   0.49  78  102  11  16 37.86 49.51  5.34 7.77       0.88   \\n\",\n",
    "       \"20                   0.50  78  102  11  16 37.86 49.51  5.34 7.77       0.88   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0     0.97      0.87 0.88 0.88  \\n\",\n",
    "       \"1     0.97      0.88 0.88 0.89  \\n\",\n",
    "       \"2     0.95      0.87 0.87 0.88  \\n\",\n",
    "       \"3     0.94      0.86 0.86 0.87  \\n\",\n",
    "       \"4     0.93      0.86 0.86 0.87  \\n\",\n",
    "       \"5     0.93      0.86 0.86 0.87  \\n\",\n",
    "       \"6     0.93      0.86 0.86 0.87  \\n\",\n",
    "       \"7     0.90      0.87 0.86 0.87  \\n\",\n",
    "       \"8     0.90      0.88 0.87 0.88  \\n\",\n",
    "       \"9     0.89      0.88 0.88 0.89  \\n\",\n",
    "       \"10    0.89      0.88 0.88 0.89  \\n\",\n",
    "       \"11    0.88      0.88 0.87 0.88  \\n\",\n",
    "       \"12    0.88      0.88 0.87 0.88  \\n\",\n",
    "       \"13    0.88      0.88 0.87 0.88  \\n\",\n",
    "       \"14    0.86      0.88 0.87 0.88  \\n\",\n",
    "       \"15    0.85      0.87 0.86 0.87  \\n\",\n",
    "       \"16    0.84      0.87 0.85 0.87  \\n\",\n",
    "       \"17    0.83      0.87 0.85 0.87  \\n\",\n",
    "       \"18    0.83      0.87 0.85 0.87  \\n\",\n",
    "       \"19    0.83      0.87 0.85 0.87  \\n\",\n",
    "       \"20    0.83      0.87 0.85 0.87  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 57,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"class_perf_measures(best_random_forest_3,X_test[predictors],y_test,0.3,0.5,0.01)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 58,\n",
    "   \"id\": \"6abde95c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"random_forest_threshold_3 = 0.44\\n\",\n",
    "    \"\\n\",\n",
    "    \"random_forest_pred_3 = (best_random_forest_3.predict_proba(X_test[predictors])[:,1] >= random_forest_threshold_3).astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_used = (best_random_forest_3.feature_importances_>0).sum()\\n\",\n",
    "    \"hyperparameters = best_random_forest_3.get_params()\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = \\\"Random Forest with Selected Features 1\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,random_forest_pred_3,features_used,total_time,df_results,hyperparameters)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 59,\n",
    "   \"id\": \"2d05df19\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Model</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"      <th>Total time taken</th>\\n\",\n",
    "       \"      <th>False Negative</th>\\n\",\n",
    "       \"      <th>False Positive</th>\\n\",\n",
    "       \"      <th>Features Used</th>\\n\",\n",
    "       \"      <th>Hyperparameters</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Logistic Regression</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>Logistic Regression for Selected Features</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>kNN Classification</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>Decision tree</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>Decision Tree Cost Complexity Pruning</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>Random Forest</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>14.54</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>Random Forest with Selected Features 1</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>14.15</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"                                       Model  Accuracy  Recall  Precision  \\\\\\n\",\n",
    "       \"0                        Logistic Regression      0.88    0.86       0.88   \\n\",\n",
    "       \"1  Logistic Regression for Selected Features      0.88    0.86       0.88   \\n\",\n",
    "       \"2                         kNN Classification      0.81    0.73       0.82   \\n\",\n",
    "       \"3                              Decision tree      0.87    0.82       0.90   \\n\",\n",
    "       \"4      Decision Tree Cost Complexity Pruning      0.84    0.76       0.89   \\n\",\n",
    "       \"5                              Random Forest      0.88    0.97       0.81   \\n\",\n",
    "       \"6     Random Forest with Selected Features 1      0.88    0.86       0.87   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   AUC  Total time taken  False Negative  False Positive  Features Used  \\\\\\n\",\n",
    "       \"0 0.88              0.03              13              11             14   \\n\",\n",
    "       \"1 0.88              0.00              13              11             14   \\n\",\n",
    "       \"2 0.80              0.82              25              15             14   \\n\",\n",
    "       \"3 0.87              0.82              17               9             12   \\n\",\n",
    "       \"4 0.84              0.00              23               9              5   \\n\",\n",
    "       \"5 0.89             14.54               3              22             15   \\n\",\n",
    "       \"6 0.88             14.15              13              12             15   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                                                                                                                                                                                                                                                              Hyperparameters  \\n\",\n",
    "       \"0                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"1                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"2                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"3                                                                                                                                                                                                    {'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}  \\n\",\n",
    "       \"4  {'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}  \\n\",\n",
    "       \"5                                                                                                                                                                                        {'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}  \\n\",\n",
    "       \"6                                                                                                                                                                                        {'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 59,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 60,\n",
    "   \"id\": \"d58d633a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
    "      \"  self.best_estimator_.fit(X, y, **fit_params)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\\n\",\n",
    "      \"  _warn_prf(average, modifier, msg_start, len(result))\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>0.62</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.10</td>\\n\",\n",
    "       \"      <td>93</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>45.15</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>0.48</td>\\n\",\n",
    "       \"      <td>0.99</td>\\n\",\n",
    "       \"      <td>0.52</td>\\n\",\n",
    "       \"      <td>0.65</td>\\n\",\n",
    "       \"      <td>0.56</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>79</td>\\n\",\n",
    "       \"      <td>34</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>38.35</td>\\n\",\n",
    "       \"      <td>16.50</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>89</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>43.20</td>\\n\",\n",
    "       \"      <td>11.65</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>77</td>\\n\",\n",
    "       \"      <td>103</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>37.38</td>\\n\",\n",
    "       \"      <td>50.00</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>8.25</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.60</td>\\n\",\n",
    "       \"      <td>67</td>\\n\",\n",
    "       \"      <td>105</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>32.52</td>\\n\",\n",
    "       \"      <td>50.97</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.71</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>57</td>\\n\",\n",
    "       \"      <td>107</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>37</td>\\n\",\n",
    "       \"      <td>27.67</td>\\n\",\n",
    "       \"      <td>51.94</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>17.96</td>\\n\",\n",
    "       \"      <td>0.91</td>\\n\",\n",
    "       \"      <td>0.61</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>45</td>\\n\",\n",
    "       \"      <td>111</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>49</td>\\n\",\n",
    "       \"      <td>21.84</td>\\n\",\n",
    "       \"      <td>53.88</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>23.79</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.48</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>0.64</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>21</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>73</td>\\n\",\n",
    "       \"      <td>10.19</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>35.44</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.22</td>\\n\",\n",
    "       \"      <td>0.65</td>\\n\",\n",
    "       \"      <td>0.37</td>\\n\",\n",
    "       \"      <td>0.61</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.55</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN   FP  FN   TP%   TN%   FP%   FN%  \\\\\\n\",\n",
    "       \"0                    0.00  94    0  113   0 45.63  0.00 54.85  0.00   \\n\",\n",
    "       \"1                    0.10  93   14   99   1 45.15  6.80 48.06  0.49   \\n\",\n",
    "       \"2                    0.20  92   79   34   2 44.66 38.35 16.50  0.97   \\n\",\n",
    "       \"3                    0.30  90   89   24   4 43.69 43.20 11.65  1.94   \\n\",\n",
    "       \"4                    0.40  84   99   14  10 40.78 48.06  6.80  4.85   \\n\",\n",
    "       \"5                    0.50  77  103   10  17 37.38 50.00  4.85  8.25   \\n\",\n",
    "       \"6                    0.60  67  105    8  27 32.52 50.97  3.88 13.11   \\n\",\n",
    "       \"7                    0.70  57  107    6  37 27.67 51.94  2.91 17.96   \\n\",\n",
    "       \"8                    0.80  45  111    2  49 21.84 53.88  0.97 23.79   \\n\",\n",
    "       \"9                    0.90  21  113    0  73 10.19 54.85  0.00 35.44   \\n\",\n",
    "       \"10                   1.00   0  113    0  94  0.00 54.85  0.00 45.63   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Precision  Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0        0.45    1.00      0.45 0.62 0.50  \\n\",\n",
    "       \"1        0.48    0.99      0.52 0.65 0.56  \\n\",\n",
    "       \"2        0.73    0.98      0.83 0.84 0.84  \\n\",\n",
    "       \"3        0.79    0.96      0.86 0.87 0.87  \\n\",\n",
    "       \"4        0.86    0.89      0.88 0.88 0.89  \\n\",\n",
    "       \"5        0.89    0.82      0.87 0.85 0.86  \\n\",\n",
    "       \"6        0.89    0.71      0.83 0.79 0.82  \\n\",\n",
    "       \"7        0.91    0.61      0.79 0.73 0.78  \\n\",\n",
    "       \"8        0.96    0.48      0.75 0.64 0.73  \\n\",\n",
    "       \"9        1.00    0.22      0.65 0.37 0.61  \\n\",\n",
    "       \"10       0.00    0.00      0.55 0.00 0.50  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 60,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"random_forest_4 = RandomForestClassifier(random_state=1)\\n\",\n",
    "    \"param_grid = { \\n\",\n",
    "    \"    'max_depth': np.arange(1,100,3)\\n\",\n",
    "    \"}\\n\",\n",
    "    \"random_forest_grid_search_4 = GridSearchCV(estimator=random_forest_4, param_grid=param_grid, cv= 5,scoring=['accuracy','recall','precision','roc_auc'],\\n\",\n",
    "    \"                          refit='roc_auc')\\n\",\n",
    "    \"random_forest_grid_search_4.fit(X_train[predictors], y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_random_forest_4 = random_forest_grid_search_4.best_estimator_\\n\",\n",
    "    \"total_time = time.time() - start_time\\n\",\n",
    "    \"\\n\",\n",
    "    \"class_perf_measures(best_random_forest_4,X_test[predictors],y_test,0,1,0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 61,\n",
    "   \"id\": \"cb2c6f83\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>79</td>\\n\",\n",
    "       \"      <td>34</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>38.35</td>\\n\",\n",
    "       \"      <td>16.50</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.21</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>82</td>\\n\",\n",
    "       \"      <td>31</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>39.81</td>\\n\",\n",
    "       \"      <td>15.05</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.22</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>83</td>\\n\",\n",
    "       \"      <td>30</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>40.29</td>\\n\",\n",
    "       \"      <td>14.56</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.75</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.23</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>28</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>13.59</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.24</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>28</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>13.59</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.25</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>28</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>13.59</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.26</td>\\n\",\n",
    "       \"      <td>92</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>44.66</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.98</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.27</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.28</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.29</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>89</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>43.20</td>\\n\",\n",
    "       \"      <td>11.65</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>11</th>\\n\",\n",
    "       \"      <td>0.31</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>89</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>43.20</td>\\n\",\n",
    "       \"      <td>11.65</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>12</th>\\n\",\n",
    "       \"      <td>0.32</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>13</th>\\n\",\n",
    "       \"      <td>0.33</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>14</th>\\n\",\n",
    "       \"      <td>0.34</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>93</td>\\n\",\n",
    "       \"      <td>20</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>45.15</td>\\n\",\n",
    "       \"      <td>9.71</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>15</th>\\n\",\n",
    "       \"      <td>0.35</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>9.22</td>\\n\",\n",
    "       \"      <td>1.94</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.96</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>16</th>\\n\",\n",
    "       \"      <td>0.36</td>\\n\",\n",
    "       \"      <td>89</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>43.20</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>9.22</td>\\n\",\n",
    "       \"      <td>2.43</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.95</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>17</th>\\n\",\n",
    "       \"      <td>0.37</td>\\n\",\n",
    "       \"      <td>88</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>42.72</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>9.22</td>\\n\",\n",
    "       \"      <td>2.91</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.94</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>18</th>\\n\",\n",
    "       \"      <td>0.38</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>96</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>46.60</td>\\n\",\n",
    "       \"      <td>8.25</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>19</th>\\n\",\n",
    "       \"      <td>0.39</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>98</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>47.57</td>\\n\",\n",
    "       \"      <td>7.28</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>20</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>21</th>\\n\",\n",
    "       \"      <td>0.41</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>99</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>48.06</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>22</th>\\n\",\n",
    "       \"      <td>0.42</td>\\n\",\n",
    "       \"      <td>84</td>\\n\",\n",
    "       \"      <td>101</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.78</td>\\n\",\n",
    "       \"      <td>49.03</td>\\n\",\n",
    "       \"      <td>5.83</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>23</th>\\n\",\n",
    "       \"      <td>0.43</td>\\n\",\n",
    "       \"      <td>83</td>\\n\",\n",
    "       \"      <td>101</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>40.29</td>\\n\",\n",
    "       \"      <td>49.03</td>\\n\",\n",
    "       \"      <td>5.83</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>24</th>\\n\",\n",
    "       \"      <td>0.44</td>\\n\",\n",
    "       \"      <td>82</td>\\n\",\n",
    "       \"      <td>101</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>39.81</td>\\n\",\n",
    "       \"      <td>49.03</td>\\n\",\n",
    "       \"      <td>5.83</td>\\n\",\n",
    "       \"      <td>5.83</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>25</th>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>80</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>38.83</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>6.80</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>26</th>\\n\",\n",
    "       \"      <td>0.46</td>\\n\",\n",
    "       \"      <td>79</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>38.35</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>7.28</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>27</th>\\n\",\n",
    "       \"      <td>0.47</td>\\n\",\n",
    "       \"      <td>79</td>\\n\",\n",
    "       \"      <td>103</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>38.35</td>\\n\",\n",
    "       \"      <td>50.00</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>7.28</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>28</th>\\n\",\n",
    "       \"      <td>0.48</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>103</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>50.00</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>29</th>\\n\",\n",
    "       \"      <td>0.49</td>\\n\",\n",
    "       \"      <td>77</td>\\n\",\n",
    "       \"      <td>103</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>37.38</td>\\n\",\n",
    "       \"      <td>50.00</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>8.25</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>30</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>77</td>\\n\",\n",
    "       \"      <td>103</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>37.38</td>\\n\",\n",
    "       \"      <td>50.00</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>8.25</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN  FP  FN   TP%   TN%   FP%  FN%  Precision  \\\\\\n\",\n",
    "       \"0                    0.20  92   79  34   2 44.66 38.35 16.50 0.97       0.73   \\n\",\n",
    "       \"1                    0.21  92   82  31   2 44.66 39.81 15.05 0.97       0.75   \\n\",\n",
    "       \"2                    0.22  92   83  30   2 44.66 40.29 14.56 0.97       0.75   \\n\",\n",
    "       \"3                    0.23  92   85  28   2 44.66 41.26 13.59 0.97       0.77   \\n\",\n",
    "       \"4                    0.24  92   85  28   2 44.66 41.26 13.59 0.97       0.77   \\n\",\n",
    "       \"5                    0.25  92   85  28   2 44.66 41.26 13.59 0.97       0.77   \\n\",\n",
    "       \"6                    0.26  92   86  27   2 44.66 41.75 13.11 0.97       0.77   \\n\",\n",
    "       \"7                    0.27  91   86  27   3 44.17 41.75 13.11 1.46       0.77   \\n\",\n",
    "       \"8                    0.28  91   86  27   3 44.17 41.75 13.11 1.46       0.77   \\n\",\n",
    "       \"9                    0.29  90   86  27   4 43.69 41.75 13.11 1.94       0.77   \\n\",\n",
    "       \"10                   0.30  90   89  24   4 43.69 43.20 11.65 1.94       0.79   \\n\",\n",
    "       \"11                   0.31  90   89  24   4 43.69 43.20 11.65 1.94       0.79   \\n\",\n",
    "       \"12                   0.32  90   90  23   4 43.69 43.69 11.17 1.94       0.80   \\n\",\n",
    "       \"13                   0.33  90   91  22   4 43.69 44.17 10.68 1.94       0.80   \\n\",\n",
    "       \"14                   0.34  90   93  20   4 43.69 45.15  9.71 1.94       0.82   \\n\",\n",
    "       \"15                   0.35  90   94  19   4 43.69 45.63  9.22 1.94       0.83   \\n\",\n",
    "       \"16                   0.36  89   94  19   5 43.20 45.63  9.22 2.43       0.82   \\n\",\n",
    "       \"17                   0.37  88   94  19   6 42.72 45.63  9.22 2.91       0.82   \\n\",\n",
    "       \"18                   0.38  87   96  17   7 42.23 46.60  8.25 3.40       0.84   \\n\",\n",
    "       \"19                   0.39  86   98  15   8 41.75 47.57  7.28 3.88       0.85   \\n\",\n",
    "       \"20                   0.40  84   99  14  10 40.78 48.06  6.80 4.85       0.86   \\n\",\n",
    "       \"21                   0.41  84   99  14  10 40.78 48.06  6.80 4.85       0.86   \\n\",\n",
    "       \"22                   0.42  84  101  12  10 40.78 49.03  5.83 4.85       0.88   \\n\",\n",
    "       \"23                   0.43  83  101  12  11 40.29 49.03  5.83 5.34       0.87   \\n\",\n",
    "       \"24                   0.44  82  101  12  12 39.81 49.03  5.83 5.83       0.87   \\n\",\n",
    "       \"25                   0.45  80  102  11  14 38.83 49.51  5.34 6.80       0.88   \\n\",\n",
    "       \"26                   0.46  79  102  11  15 38.35 49.51  5.34 7.28       0.88   \\n\",\n",
    "       \"27                   0.47  79  103  10  15 38.35 50.00  4.85 7.28       0.89   \\n\",\n",
    "       \"28                   0.48  78  103  10  16 37.86 50.00  4.85 7.77       0.89   \\n\",\n",
    "       \"29                   0.49  77  103  10  17 37.38 50.00  4.85 8.25       0.89   \\n\",\n",
    "       \"30                   0.50  77  103  10  17 37.38 50.00  4.85 8.25       0.89   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0     0.98      0.83 0.84 0.84  \\n\",\n",
    "       \"1     0.98      0.84 0.85 0.85  \\n\",\n",
    "       \"2     0.98      0.85 0.85 0.86  \\n\",\n",
    "       \"3     0.98      0.86 0.86 0.86  \\n\",\n",
    "       \"4     0.98      0.86 0.86 0.86  \\n\",\n",
    "       \"5     0.98      0.86 0.86 0.86  \\n\",\n",
    "       \"6     0.98      0.86 0.86 0.87  \\n\",\n",
    "       \"7     0.97      0.86 0.86 0.86  \\n\",\n",
    "       \"8     0.97      0.86 0.86 0.86  \\n\",\n",
    "       \"9     0.96      0.85 0.85 0.86  \\n\",\n",
    "       \"10    0.96      0.86 0.87 0.87  \\n\",\n",
    "       \"11    0.96      0.86 0.87 0.87  \\n\",\n",
    "       \"12    0.96      0.87 0.87 0.88  \\n\",\n",
    "       \"13    0.96      0.87 0.87 0.88  \\n\",\n",
    "       \"14    0.96      0.88 0.88 0.89  \\n\",\n",
    "       \"15    0.96      0.89 0.89 0.90  \\n\",\n",
    "       \"16    0.95      0.88 0.88 0.89  \\n\",\n",
    "       \"17    0.94      0.88 0.88 0.88  \\n\",\n",
    "       \"18    0.93      0.88 0.88 0.89  \\n\",\n",
    "       \"19    0.92      0.89 0.88 0.89  \\n\",\n",
    "       \"20    0.89      0.88 0.88 0.89  \\n\",\n",
    "       \"21    0.89      0.88 0.88 0.89  \\n\",\n",
    "       \"22    0.89      0.89 0.88 0.89  \\n\",\n",
    "       \"23    0.88      0.89 0.88 0.89  \\n\",\n",
    "       \"24    0.87      0.88 0.87 0.88  \\n\",\n",
    "       \"25    0.85      0.88 0.86 0.88  \\n\",\n",
    "       \"26    0.84      0.87 0.86 0.87  \\n\",\n",
    "       \"27    0.84      0.88 0.86 0.88  \\n\",\n",
    "       \"28    0.83      0.87 0.86 0.87  \\n\",\n",
    "       \"29    0.82      0.87 0.85 0.86  \\n\",\n",
    "       \"30    0.82      0.87 0.85 0.86  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 61,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"class_perf_measures(best_random_forest_4,X_test[predictors],y_test,0.2,0.5,0.01)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 62,\n",
    "   \"id\": \"34dc0d08\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{'ccp_alpha': 0.0, 'max_depth': 4, 'min_impurity_decrease': 0.0, 'min_samples_split': 2}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"random_forest_threshold_4 = 0.47\\n\",\n",
    "    \"\\n\",\n",
    "    \"random_forest_pred_4 = (best_random_forest_4.predict_proba(X_test[predictors])[:,1] >= random_forest_threshold_3).astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_used = (best_random_forest_4.feature_importances_>0).sum()\\n\",\n",
    "    \"hyperparameters = best_random_forest_4.get_params()\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = \\\"Random Forest with Selected Features 2\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,random_forest_pred_4,features_used,total_time,df_results,hyperparameters)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 63,\n",
    "   \"id\": \"b8bd8f43\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Model</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"      <th>Total time taken</th>\\n\",\n",
    "       \"      <th>False Negative</th>\\n\",\n",
    "       \"      <th>False Positive</th>\\n\",\n",
    "       \"      <th>Features Used</th>\\n\",\n",
    "       \"      <th>Hyperparameters</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Logistic Regression</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>Logistic Regression for Selected Features</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>kNN Classification</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>Decision tree</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>Decision Tree Cost Complexity Pruning</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>Random Forest</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>14.54</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>Random Forest with Selected Features 1</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>14.15</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>Random Forest with Selected Features 2</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>30.75</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.0, 'max_depth': 4, 'min_impurity_decrease': 0.0, 'min_samples_split': 2}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"                                       Model  Accuracy  Recall  Precision  \\\\\\n\",\n",
    "       \"0                        Logistic Regression      0.88    0.86       0.88   \\n\",\n",
    "       \"1  Logistic Regression for Selected Features      0.88    0.86       0.88   \\n\",\n",
    "       \"2                         kNN Classification      0.81    0.73       0.82   \\n\",\n",
    "       \"3                              Decision tree      0.87    0.82       0.90   \\n\",\n",
    "       \"4      Decision Tree Cost Complexity Pruning      0.84    0.76       0.89   \\n\",\n",
    "       \"5                              Random Forest      0.88    0.97       0.81   \\n\",\n",
    "       \"6     Random Forest with Selected Features 1      0.88    0.86       0.87   \\n\",\n",
    "       \"7     Random Forest with Selected Features 2      0.88    0.87       0.87   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   AUC  Total time taken  False Negative  False Positive  Features Used  \\\\\\n\",\n",
    "       \"0 0.88              0.03              13              11             14   \\n\",\n",
    "       \"1 0.88              0.00              13              11             14   \\n\",\n",
    "       \"2 0.80              0.82              25              15             14   \\n\",\n",
    "       \"3 0.87              0.82              17               9             12   \\n\",\n",
    "       \"4 0.84              0.00              23               9              5   \\n\",\n",
    "       \"5 0.89             14.54               3              22             15   \\n\",\n",
    "       \"6 0.88             14.15              13              12             15   \\n\",\n",
    "       \"7 0.88             30.75              12              12             15   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                                                                                                                                                                                                                                                              Hyperparameters  \\n\",\n",
    "       \"0                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"1                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"2                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"3                                                                                                                                                                                                    {'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}  \\n\",\n",
    "       \"4  {'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}  \\n\",\n",
    "       \"5                                                                                                                                                                                        {'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}  \\n\",\n",
    "       \"6                                                                                                                                                                                        {'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}  \\n\",\n",
    "       \"7                                                                                                                                                                                                    {'ccp_alpha': 0.0, 'max_depth': 4, 'min_impurity_decrease': 0.0, 'min_samples_split': 2}  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 63,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"f90ce084\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 5.5 Artificial Neural Networks<a class=\\\"anchor\\\" id=\\\"fifth-model\\\"></a>\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"*  [Go to ML Models](#maclrn)\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\\n\",\n",
    "    \"\\n\",\n",
    "    \"<br>\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src = 'https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png'\\n\",\n",
    "    \"     width = \\\"700\\\"/>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Neural networks rely on training data to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the most well-known neural networks is Google’s search algorithm.\\n\",\n",
    "    \"\\n\",\n",
    "    \"For more details: https://www.ibm.com/cloud/learn/neural-networks\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"99a3379d\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Using all the features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"acb6184c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"- In the first model, we shall use only 2 hidden layers to keep it small and simple\\n\",\n",
    "    \"- Activation function used is ReLu as is the most common activation function, other functions will be tried out later\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 64,\n",
    "   \"id\": \"80343723\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"ANN Model 1\\n\",\n",
    "      \"Accuracy:  0.855\\n\",\n",
    "      \"Recall:  0.809\\n\",\n",
    "      \"Precision:  0.864\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"nural_net = MLPClassifier(hidden_layer_sizes=[35, 20], alpha=0.001, solver='adam', activation='relu',random_state=1)\\n\",\n",
    "    \"nural_net.fit(X_train, y_train)\\n\",\n",
    "    \"nural_net_pred = nural_net.predict(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ANN Model 1\\\")\\n\",\n",
    "    \"print(\\\"Accuracy: \\\",round(metrics.accuracy_score(y_test, nural_net_pred),3))\\n\",\n",
    "    \"print(\\\"Recall: \\\",round(metrics.recall_score(y_test, nural_net_pred),3))\\n\",\n",
    "    \"print(\\\"Precision: \\\",round(metrics.precision_score(y_test, nural_net_pred),3))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"962f867e\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"- For the second model, we shall increase the size to get higher Accuracy \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 65,\n",
    "   \"id\": \"de054a73\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"ANN Model 2\\n\",\n",
    "      \"Accuracy:  0.855\\n\",\n",
    "      \"Recall:  0.798\\n\",\n",
    "      \"Precision:  0.872\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"nural_net_2 = MLPClassifier(hidden_layer_sizes=[35, 20, 20, 20], alpha=0.001, solver='adam', activation='relu',random_state=1)\\n\",\n",
    "    \"nural_net_2.fit(X_train, y_train)\\n\",\n",
    "    \"nural_net_pred_2 = nural_net_2.predict(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ANN Model 2\\\")\\n\",\n",
    "    \"print(\\\"Accuracy: \\\",round(metrics.accuracy_score(y_test, nural_net_pred_2),3))\\n\",\n",
    "    \"print(\\\"Recall: \\\",round(metrics.recall_score(y_test, nural_net_pred_2),3))\\n\",\n",
    "    \"print(\\\"Precision: \\\",round(metrics.precision_score(y_test, nural_net_pred_2),3))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e3732093\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"- Now that we have run a couple of models, let's try using different hyperparameters\\n\",\n",
    "    \"- We shall use gridsearch to create 3 models with hidden layers size: (50,50,50),(50,100,50),(100) with cv=5 \\n\",\n",
    "    \"- Best model shall be selected based on AUC\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 66,\n",
    "   \"id\": \"71355842\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"--- 13.297671556472778 seconds ---\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"scoring = {\\\"AUC\\\": \\\"roc_auc\\\", \\\"Accuracy\\\": 'accuracy','recall':'recall','precision':'precision'}\\n\",\n",
    "    \"grid = {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)]}\\n\",\n",
    "    \"ANNGrid = GridSearchCV(estimator=MLPClassifier(random_state=1021),cv=5,scoring=scoring, refit='AUC',param_grid=grid)\\n\",\n",
    "    \"ANNGrid_results = ANNGrid.fit(X_train, y_train).cv_results_\\n\",\n",
    "    \"ANNGridresults=pd.DataFrame(ANNGrid_results)\\n\",\n",
    "    \"total_time = time.time() - start_time\\n\",\n",
    "    \"print(\\\"--- %s seconds ---\\\" % (time.time() - start_time))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 67,\n",
    "   \"id\": \"52457474\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>param_hidden_layer_sizes</th>\\n\",\n",
    "       \"      <th>params</th>\\n\",\n",
    "       \"      <th>mean_test_Accuracy</th>\\n\",\n",
    "       \"      <th>mean_test_AUC</th>\\n\",\n",
    "       \"      <th>rank_test_AUC</th>\\n\",\n",
    "       \"      <th>mean_test_recall</th>\\n\",\n",
    "       \"      <th>mean_test_precision</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>(50, 50, 50)</td>\\n\",\n",
    "       \"      <td>{'hidden_layer_sizes': (50, 50, 50)}</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>(50, 100, 50)</td>\\n\",\n",
    "       \"      <td>{'hidden_layer_sizes': (50, 100, 50)}</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>(100,)</td>\\n\",\n",
    "       \"      <td>{'hidden_layer_sizes': (100,)}</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"  param_hidden_layer_sizes                                 params  \\\\\\n\",\n",
    "       \"0             (50, 50, 50)   {'hidden_layer_sizes': (50, 50, 50)}   \\n\",\n",
    "       \"1            (50, 100, 50)  {'hidden_layer_sizes': (50, 100, 50)}   \\n\",\n",
    "       \"2                   (100,)         {'hidden_layer_sizes': (100,)}   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   mean_test_Accuracy  mean_test_AUC  rank_test_AUC  mean_test_recall  \\\\\\n\",\n",
    "       \"0                0.80           0.89              3              0.79   \\n\",\n",
    "       \"1                0.83           0.90              2              0.81   \\n\",\n",
    "       \"2                0.85           0.92              1              0.83   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   mean_test_precision  \\n\",\n",
    "       \"0                 0.77  \\n\",\n",
    "       \"1                 0.80  \\n\",\n",
    "       \"2                 0.84  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 67,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"ANNGridresults[['param_hidden_layer_sizes','params','mean_test_Accuracy','mean_test_AUC','rank_test_AUC','mean_test_recall','mean_test_precision']]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 68,\n",
    "   \"id\": \"48ddcd50\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>26</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>12.62</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.21</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>26</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>12.62</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.22</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>89</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>43.20</td>\\n\",\n",
    "       \"      <td>11.65</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.23</td>\\n\",\n",
    "       \"      <td>87</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>42.23</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.93</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.24</td>\\n\",\n",
    "       \"      <td>86</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>41.75</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.92</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.25</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.26</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>10.68</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"      <td>0.85</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.27</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>9.22</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.28</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>9.22</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.29</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>9.22</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>85</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>41.26</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>9.22</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP  TN  FP  FN   TP%   TN%   FP%  FN%  Precision  \\\\\\n\",\n",
    "       \"0                    0.20  87  87  26   7 42.23 42.23 12.62 3.40       0.77   \\n\",\n",
    "       \"1                    0.21  87  87  26   7 42.23 42.23 12.62 3.40       0.77   \\n\",\n",
    "       \"2                    0.22  87  89  24   7 42.23 43.20 11.65 3.40       0.78   \\n\",\n",
    "       \"3                    0.23  87  90  23   7 42.23 43.69 11.17 3.40       0.79   \\n\",\n",
    "       \"4                    0.24  86  90  23   8 41.75 43.69 11.17 3.88       0.79   \\n\",\n",
    "       \"5                    0.25  85  91  22   9 41.26 44.17 10.68 4.37       0.79   \\n\",\n",
    "       \"6                    0.26  85  91  22   9 41.26 44.17 10.68 4.37       0.79   \\n\",\n",
    "       \"7                    0.27  85  94  19   9 41.26 45.63  9.22 4.37       0.82   \\n\",\n",
    "       \"8                    0.28  85  94  19   9 41.26 45.63  9.22 4.37       0.82   \\n\",\n",
    "       \"9                    0.29  85  94  19   9 41.26 45.63  9.22 4.37       0.82   \\n\",\n",
    "       \"10                   0.30  85  94  19   9 41.26 45.63  9.22 4.37       0.82   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0     0.93      0.84 0.84 0.85  \\n\",\n",
    "       \"1     0.93      0.84 0.84 0.85  \\n\",\n",
    "       \"2     0.93      0.85 0.85 0.86  \\n\",\n",
    "       \"3     0.93      0.86 0.85 0.86  \\n\",\n",
    "       \"4     0.92      0.85 0.85 0.86  \\n\",\n",
    "       \"5     0.90      0.85 0.85 0.85  \\n\",\n",
    "       \"6     0.90      0.85 0.85 0.85  \\n\",\n",
    "       \"7     0.90      0.86 0.86 0.87  \\n\",\n",
    "       \"8     0.90      0.86 0.86 0.87  \\n\",\n",
    "       \"9     0.90      0.86 0.86 0.87  \\n\",\n",
    "       \"10    0.90      0.86 0.86 0.87  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 68,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"best_neural_net = ANNGrid.best_estimator_\\n\",\n",
    "    \"class_perf_measures(best_neural_net,X_test,y_test,0.2,0.3,0.01)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"b7cf7a7a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"- From the grid search, we found the model with 100 hidden layers gives the highest AUC\\n\",\n",
    "    \"- For that model we checked performance by varying the threshold and found that 0.27 probability threshold is giving the best combination of Precision and Accuracy.\\n\",\n",
    "    \"- We shall now add the results of this model in df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 69,\n",
    "   \"id\": \"9f10224a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{'activation': 'relu', 'hidden_layer_sizes': (100,)}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"ann_threshold = 0.27\\n\",\n",
    "    \"\\n\",\n",
    "    \"ann_pred = (best_neural_net.predict_proba(X_test)[:,1] >= ann_threshold).astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_used = len(X_train)\\n\",\n",
    "    \"hyperparameters = best_neural_net.get_params()\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = \\\"Artifical Neural Nets\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,ann_pred,features_used,total_time,df_results,hyperparameters)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"b6132ce6\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Using selected features\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Hyperparameter tuning took considerable amount of time to create all the models\\n\",\n",
    "    \"- Most of the models with activation function as tanh were performing better than relu\\n\",\n",
    "    \"- Thus to reduce the time ill be using only tanh as activation function \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 70,\n",
    "   \"id\": \"67381ec3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\\n\",\n",
    "      \"  y = column_or_1d(y, warn=True)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"--- 12.752894163131714 seconds ---\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"C:\\\\Users\\\\bhags\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\neural_network\\\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\\n\",\n",
    "      \"  warnings.warn(\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"scoring = {\\\"AUC\\\": \\\"roc_auc\\\", \\\"Accuracy\\\": 'accuracy','recall':'recall','precision':'precision'}\\n\",\n",
    "    \"grid = {'hidden_layer_sizes': [(35,25,30,25),(35,25,20,25),(25,35,30,25), (25,35,20,25)]}\\n\",\n",
    "    \"ANNGrid_2 = GridSearchCV(estimator=MLPClassifier(random_state=1021),cv=5,scoring=scoring, refit='AUC',param_grid=grid)\\n\",\n",
    "    \"ANNGrid_results_2 = ANNGrid_2.fit(X_train, y_train).cv_results_\\n\",\n",
    "    \"ANNGridresults_2=pd.DataFrame(ANNGrid_results_2)\\n\",\n",
    "    \"total_time = time.time() - start_time\\n\",\n",
    "    \"print(\\\"--- %s seconds ---\\\" % (time.time() - start_time))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 71,\n",
    "   \"id\": \"eca20ab3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>param_hidden_layer_sizes</th>\\n\",\n",
    "       \"      <th>params</th>\\n\",\n",
    "       \"      <th>mean_test_Accuracy</th>\\n\",\n",
    "       \"      <th>mean_test_AUC</th>\\n\",\n",
    "       \"      <th>rank_test_AUC</th>\\n\",\n",
    "       \"      <th>mean_test_recall</th>\\n\",\n",
    "       \"      <th>mean_test_precision</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>(35, 25, 30, 25)</td>\\n\",\n",
    "       \"      <td>{'hidden_layer_sizes': (35, 25, 30, 25)}</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>(35, 25, 20, 25)</td>\\n\",\n",
    "       \"      <td>{'hidden_layer_sizes': (35, 25, 20, 25)}</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>(25, 35, 30, 25)</td>\\n\",\n",
    "       \"      <td>{'hidden_layer_sizes': (25, 35, 30, 25)}</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>(25, 35, 20, 25)</td>\\n\",\n",
    "       \"      <td>{'hidden_layer_sizes': (25, 35, 20, 25)}</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"  param_hidden_layer_sizes                                    params  \\\\\\n\",\n",
    "       \"0         (35, 25, 30, 25)  {'hidden_layer_sizes': (35, 25, 30, 25)}   \\n\",\n",
    "       \"1         (35, 25, 20, 25)  {'hidden_layer_sizes': (35, 25, 20, 25)}   \\n\",\n",
    "       \"2         (25, 35, 30, 25)  {'hidden_layer_sizes': (25, 35, 30, 25)}   \\n\",\n",
    "       \"3         (25, 35, 20, 25)  {'hidden_layer_sizes': (25, 35, 20, 25)}   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   mean_test_Accuracy  mean_test_AUC  rank_test_AUC  mean_test_recall  \\\\\\n\",\n",
    "       \"0                0.83           0.90              2              0.81   \\n\",\n",
    "       \"1                0.82           0.90              3              0.80   \\n\",\n",
    "       \"2                0.82           0.89              4              0.79   \\n\",\n",
    "       \"3                0.83           0.90              1              0.81   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   mean_test_precision  \\n\",\n",
    "       \"0                 0.81  \\n\",\n",
    "       \"1                 0.81  \\n\",\n",
    "       \"2                 0.80  \\n\",\n",
    "       \"3                 0.81  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 71,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"ANNGridresults_2[['param_hidden_layer_sizes','params','mean_test_Accuracy','mean_test_AUC','rank_test_AUC','mean_test_recall','mean_test_precision']]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 72,\n",
    "   \"id\": \"3ff49803\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Probability Threshold</th>\\n\",\n",
    "       \"      <th>TP</th>\\n\",\n",
    "       \"      <th>TN</th>\\n\",\n",
    "       \"      <th>FP</th>\\n\",\n",
    "       \"      <th>FN</th>\\n\",\n",
    "       \"      <th>TP%</th>\\n\",\n",
    "       \"      <th>TN%</th>\\n\",\n",
    "       \"      <th>FP%</th>\\n\",\n",
    "       \"      <th>FN%</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>F1</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>94</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>45.63</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.45</td>\\n\",\n",
    "       \"      <td>0.62</td>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>0.10</td>\\n\",\n",
    "       \"      <td>78</td>\\n\",\n",
    "       \"      <td>90</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>37.86</td>\\n\",\n",
    "       \"      <td>43.69</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>0.77</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>0.20</td>\\n\",\n",
    "       \"      <td>76</td>\\n\",\n",
    "       \"      <td>95</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>36.89</td>\\n\",\n",
    "       \"      <td>46.12</td>\\n\",\n",
    "       \"      <td>8.74</td>\\n\",\n",
    "       \"      <td>8.74</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>0.30</td>\\n\",\n",
    "       \"      <td>71</td>\\n\",\n",
    "       \"      <td>97</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>34.47</td>\\n\",\n",
    "       \"      <td>47.09</td>\\n\",\n",
    "       \"      <td>7.77</td>\\n\",\n",
    "       \"      <td>11.17</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.78</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>0.40</td>\\n\",\n",
    "       \"      <td>70</td>\\n\",\n",
    "       \"      <td>100</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>33.98</td>\\n\",\n",
    "       \"      <td>48.54</td>\\n\",\n",
    "       \"      <td>6.31</td>\\n\",\n",
    "       \"      <td>11.65</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>0.50</td>\\n\",\n",
    "       \"      <td>70</td>\\n\",\n",
    "       \"      <td>102</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>33.98</td>\\n\",\n",
    "       \"      <td>49.51</td>\\n\",\n",
    "       \"      <td>5.34</td>\\n\",\n",
    "       \"      <td>11.65</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>0.60</td>\\n\",\n",
    "       \"      <td>70</td>\\n\",\n",
    "       \"      <td>103</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>33.98</td>\\n\",\n",
    "       \"      <td>50.00</td>\\n\",\n",
    "       \"      <td>4.85</td>\\n\",\n",
    "       \"      <td>11.65</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>68</td>\\n\",\n",
    "       \"      <td>104</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>26</td>\\n\",\n",
    "       \"      <td>33.01</td>\\n\",\n",
    "       \"      <td>50.49</td>\\n\",\n",
    "       \"      <td>4.37</td>\\n\",\n",
    "       \"      <td>12.62</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.72</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>67</td>\\n\",\n",
    "       \"      <td>105</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>27</td>\\n\",\n",
    "       \"      <td>32.52</td>\\n\",\n",
    "       \"      <td>50.97</td>\\n\",\n",
    "       \"      <td>3.88</td>\\n\",\n",
    "       \"      <td>13.11</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.71</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>66</td>\\n\",\n",
    "       \"      <td>106</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>28</td>\\n\",\n",
    "       \"      <td>32.04</td>\\n\",\n",
    "       \"      <td>51.46</td>\\n\",\n",
    "       \"      <td>3.40</td>\\n\",\n",
    "       \"      <td>13.59</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.70</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>0.79</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>113</td>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>91</td>\\n\",\n",
    "       \"      <td>1.46</td>\\n\",\n",
    "       \"      <td>54.85</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>44.17</td>\\n\",\n",
    "       \"      <td>1.00</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>0.56</td>\\n\",\n",
    "       \"      <td>0.06</td>\\n\",\n",
    "       \"      <td>0.52</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    Probability Threshold  TP   TN   FP  FN   TP%   TN%   FP%   FN%  \\\\\\n\",\n",
    "       \"0                    0.00  94    0  113   0 45.63  0.00 54.85  0.00   \\n\",\n",
    "       \"1                    0.10  78   90   23  16 37.86 43.69 11.17  7.77   \\n\",\n",
    "       \"2                    0.20  76   95   18  18 36.89 46.12  8.74  8.74   \\n\",\n",
    "       \"3                    0.30  71   97   16  23 34.47 47.09  7.77 11.17   \\n\",\n",
    "       \"4                    0.40  70  100   13  24 33.98 48.54  6.31 11.65   \\n\",\n",
    "       \"5                    0.50  70  102   11  24 33.98 49.51  5.34 11.65   \\n\",\n",
    "       \"6                    0.60  70  103   10  24 33.98 50.00  4.85 11.65   \\n\",\n",
    "       \"7                    0.70  68  104    9  26 33.01 50.49  4.37 12.62   \\n\",\n",
    "       \"8                    0.80  67  105    8  27 32.52 50.97  3.88 13.11   \\n\",\n",
    "       \"9                    0.90  66  106    7  28 32.04 51.46  3.40 13.59   \\n\",\n",
    "       \"10                   1.00   3  113    0  91  1.46 54.85  0.00 44.17   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    Precision  Recall  Accuracy   F1  AUC  \\n\",\n",
    "       \"0        0.45    1.00      0.45 0.62 0.50  \\n\",\n",
    "       \"1        0.77    0.83      0.81 0.80 0.81  \\n\",\n",
    "       \"2        0.81    0.81      0.83 0.81 0.82  \\n\",\n",
    "       \"3        0.82    0.76      0.81 0.78 0.81  \\n\",\n",
    "       \"4        0.84    0.74      0.82 0.79 0.81  \\n\",\n",
    "       \"5        0.86    0.74      0.83 0.80 0.82  \\n\",\n",
    "       \"6        0.88    0.74      0.84 0.80 0.83  \\n\",\n",
    "       \"7        0.88    0.72      0.83 0.80 0.82  \\n\",\n",
    "       \"8        0.89    0.71      0.83 0.79 0.82  \\n\",\n",
    "       \"9        0.90    0.70      0.83 0.79 0.82  \\n\",\n",
    "       \"10       1.00    0.03      0.56 0.06 0.52  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 72,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"best_neural_net_2 = ANNGrid_2.best_estimator_\\n\",\n",
    "    \"class_perf_measures(best_neural_net_2,X_test,y_test,0,1,0.1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 73,\n",
    "   \"id\": \"aa4c5659\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{'activation': 'relu', 'hidden_layer_sizes': (25, 35, 20, 25)}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"ann_threshold_2 = 0.60\\n\",\n",
    "    \"\\n\",\n",
    "    \"ann_pred_2 = (best_neural_net_2.predict_proba(X_test)[:,1] >= ann_threshold_2).astype(int)\\n\",\n",
    "    \"\\n\",\n",
    "    \"features_used = len(X_train)\\n\",\n",
    "    \"hyperparameters = best_neural_net_2.get_params()\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = \\\"Artifical Neural Nets with Selected Features\\\"\\n\",\n",
    "    \"df_results = update_results_df(model,y_test,ann_pred_2,features_used,total_time,df_results,hyperparameters)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 74,\n",
    "   \"id\": \"d99cc3dd\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>Model</th>\\n\",\n",
    "       \"      <th>Accuracy</th>\\n\",\n",
    "       \"      <th>Recall</th>\\n\",\n",
    "       \"      <th>Precision</th>\\n\",\n",
    "       \"      <th>AUC</th>\\n\",\n",
    "       \"      <th>Total time taken</th>\\n\",\n",
    "       \"      <th>False Negative</th>\\n\",\n",
    "       \"      <th>False Positive</th>\\n\",\n",
    "       \"      <th>Features Used</th>\\n\",\n",
    "       \"      <th>Hyperparameters</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>Logistic Regression</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.03</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>Logistic Regression for Selected Features</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>kNN Classification</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.73</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.80</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>25</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>nan</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>Decision tree</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>Decision Tree Cost Complexity Pruning</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.76</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.00</td>\\n\",\n",
    "       \"      <td>23</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>Random Forest</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.97</td>\\n\",\n",
    "       \"      <td>0.81</td>\\n\",\n",
    "       \"      <td>0.89</td>\\n\",\n",
    "       \"      <td>14.54</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>22</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>Random Forest with Selected Features 1</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>14.15</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>Random Forest with Selected Features 2</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>30.75</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'ccp_alpha': 0.0, 'max_depth': 4, 'min_impurity_decrease': 0.0, 'min_samples_split': 2}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>Artifical Neural Nets</td>\\n\",\n",
    "       \"      <td>0.86</td>\\n\",\n",
    "       \"      <td>0.90</td>\\n\",\n",
    "       \"      <td>0.82</td>\\n\",\n",
    "       \"      <td>0.87</td>\\n\",\n",
    "       \"      <td>13.30</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>483</td>\\n\",\n",
    "       \"      <td>{'activation': 'relu', 'hidden_layer_sizes': (100,)}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>Artifical Neural Nets with Selected Features</td>\\n\",\n",
    "       \"      <td>0.84</td>\\n\",\n",
    "       \"      <td>0.74</td>\\n\",\n",
    "       \"      <td>0.88</td>\\n\",\n",
    "       \"      <td>0.83</td>\\n\",\n",
    "       \"      <td>12.75</td>\\n\",\n",
    "       \"      <td>24</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>483</td>\\n\",\n",
    "       \"      <td>{'activation': 'relu', 'hidden_layer_sizes': (25, 35, 20, 25)}</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"                                          Model  Accuracy  Recall  Precision  \\\\\\n\",\n",
    "       \"0                           Logistic Regression      0.88    0.86       0.88   \\n\",\n",
    "       \"1     Logistic Regression for Selected Features      0.88    0.86       0.88   \\n\",\n",
    "       \"2                            kNN Classification      0.81    0.73       0.82   \\n\",\n",
    "       \"3                                 Decision tree      0.87    0.82       0.90   \\n\",\n",
    "       \"4         Decision Tree Cost Complexity Pruning      0.84    0.76       0.89   \\n\",\n",
    "       \"5                                 Random Forest      0.88    0.97       0.81   \\n\",\n",
    "       \"6        Random Forest with Selected Features 1      0.88    0.86       0.87   \\n\",\n",
    "       \"7        Random Forest with Selected Features 2      0.88    0.87       0.87   \\n\",\n",
    "       \"8                         Artifical Neural Nets      0.86    0.90       0.82   \\n\",\n",
    "       \"9  Artifical Neural Nets with Selected Features      0.84    0.74       0.88   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   AUC  Total time taken  False Negative  False Positive  Features Used  \\\\\\n\",\n",
    "       \"0 0.88              0.03              13              11             14   \\n\",\n",
    "       \"1 0.88              0.00              13              11             14   \\n\",\n",
    "       \"2 0.80              0.82              25              15             14   \\n\",\n",
    "       \"3 0.87              0.82              17               9             12   \\n\",\n",
    "       \"4 0.84              0.00              23               9              5   \\n\",\n",
    "       \"5 0.89             14.54               3              22             15   \\n\",\n",
    "       \"6 0.88             14.15              13              12             15   \\n\",\n",
    "       \"7 0.88             30.75              12              12             15   \\n\",\n",
    "       \"8 0.87             13.30               9              19            483   \\n\",\n",
    "       \"9 0.83             12.75              24              10            483   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                                                                                                                                                                                                                                                              Hyperparameters  \\n\",\n",
    "       \"0                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"1                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"2                                                                                                                                                                                                                                                                                         nan  \\n\",\n",
    "       \"3                                                                                                                                                                                                    {'ccp_alpha': 0.0, 'max_depth': 30, 'min_impurity_decrease': 0, 'min_samples_split': 20}  \\n\",\n",
    "       \"4  {'ccp_alpha': 0.00522251, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}  \\n\",\n",
    "       \"5                                                                                                                                                                                        {'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}  \\n\",\n",
    "       \"6                                                                                                                                                                                        {'ccp_alpha': 0.00010131, 'max_depth': 15, 'min_impurity_decrease': 0.0001, 'min_samples_split': 20}  \\n\",\n",
    "       \"7                                                                                                                                                                                                    {'ccp_alpha': 0.0, 'max_depth': 4, 'min_impurity_decrease': 0.0, 'min_samples_split': 2}  \\n\",\n",
    "       \"8                                                                                                                                                                                                                                        {'activation': 'relu', 'hidden_layer_sizes': (100,)}  \\n\",\n",
    "       \"9                                                                                                                                                                                                                              {'activation': 'relu', 'hidden_layer_sizes': (25, 35, 20, 25)}  \"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 74,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"df_results\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"3153db04\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Model Comparison:<a class=\\\"anchor\\\" id=\\\"modelcomp\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"a053d1ee\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"For our Classification models, we will be considering both the Precision and the Accuracy, as well as time taken for comparing the performance of both the models. At present, highest Precision and Accuracy is observed for the Logistic Regression model.\\n\",\n",
    "    \"\\n\",\n",
    "    \"However, we shall also look at the ROC Curve and compare the AUC for model selection. \\n\",\n",
    "    \"The ROC Curve computes the area under the curve on different probability thersholds. The model with the highest AUC will also help in considering which is as a better fit model.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"dc7d0855\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"<b>ROC Curve\\n\",\n",
    "    \"This is an evaluation metric for binary classification problems at various thresholds\\n\",\n",
    "    \"\\n\",\n",
    "    \"- ROC (Receiver Operating Characteristic) curve is a probability curve/ graph that shows the performance of a classification measure at all classification methods.\\n\",\n",
    "    \"- This curve plots 2 parameters: True Positive Rate (recall) and False Positive Rate at different classification thresholds\\n\",\n",
    "    \"- Lowering the classification threshold classifies more items as positive, thereby increasing both False Positives and True Positives\\n\",\n",
    "    \"- The receiver operating characteristic (ROC) curve, which is defined as a plot of test sensitivity as the y coordinate versus its 1-specificity or false positive rate (FPR) as the x coordinate, is an effective method of evaluating the performance of diagnostic tests.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"<b>AUC-ROC Curve\\n\",\n",
    "    \"Area Under ROC Curve \\n\",\n",
    "    \"The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes\\n\",\n",
    "    \"- For AUC = 1, classifier is able to distinguish between all the Positive and Negative class points correctly\\n\",\n",
    "    \"- However if AUC = 0, classifier predicts all the Negatives as Positives and Positives as Negatives.\\n\",\n",
    "    \"- When 0.5 < AUC < 1, there is a high chance that the classifier will be able to differentiate between the Positive class values from the Negative class values. This is so because the classifier is able to detect more number of True positives and True negatives than False positives and False negatives\\n\",\n",
    "    \"- When AUC = 0.5, classifier is not able to distinguish between Positive and Negative class points. Meaning either the classifier is predicting random class or constant class for all the data points.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"So, the higher the AUC value for a classifier, the better its ability to distinguish between positive and negative classes.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 75,\n",
    "   \"id\": \"7c59cf8b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"image/png\": \"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\\n\",\n",
    "      \"text/plain\": [\n",
    "       \"<Figure size 432x288 with 1 Axes>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {\n",
    "      \"needs_background\": \"light\"\n",
    "     },\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"image/png\": \"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\\n\",\n",
    "      \"text/plain\": [\n",
    "       \"<Figure size 432x288 with 1 Axes>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {\n",
    "      \"needs_background\": \"light\"\n",
    "     },\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"image/png\": \"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\\n\",\n",
    "      \"text/plain\": [\n",
    "       \"<Figure size 432x288 with 1 Axes>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {\n",
    "      \"needs_background\": \"light\"\n",
    "     },\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"image/png\": \"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\\n\",\n",
    "      \"text/plain\": [\n",
    "       \"<Figure size 432x288 with 1 Axes>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {\n",
    "      \"needs_background\": \"light\"\n",
    "     },\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"image/png\": \"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\\n\",\n",
    "      \"text/plain\": [\n",
    "       \"<Figure size 432x288 with 1 Axes>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {\n",
    "      \"needs_background\": \"light\"\n",
    "     },\n",
    "     \"output_type\": \"display_data\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# ROC curve for Logistic Model\\n\",\n",
    "    \"metrics.RocCurveDisplay.from_estimator(lr_model,X_test,y_test)\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ROC curve for kNN\\n\",\n",
    "    \"metrics.RocCurveDisplay.from_estimator(knn_model,X_test,y_test)\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ROC curve for Decision Tree\\n\",\n",
    "    \"fpr, tpr, thresholds = roc_curve(y_test, decision_tree_ccp_pred)\\n\",\n",
    "    \"roc_auc = auc(fpr, tpr)\\n\",\n",
    "    \"plt.plot(fpr, tpr, label='DecisionTree (AUC = %0.2f)' % roc_auc)\\n\",\n",
    "    \"#plt.plot([0, 1], [0, 1], 'k--')  # Plot the diagonal line\\n\",\n",
    "    \"plt.xlabel('False Positive Rate')\\n\",\n",
    "    \"plt.ylabel('True Positive Rate')\\n\",\n",
    "    \"#plt.title('Receiver Operating Characteristic (ROC) - Decision Tree')\\n\",\n",
    "    \"plt.legend(loc='lower right')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ROC curve for Random Forest\\n\",\n",
    "    \"fpr, tpr, thresholds = roc_curve(y_test, random_forest_pred_4)\\n\",\n",
    "    \"roc_auc = auc(fpr, tpr)\\n\",\n",
    "    \"plt.plot(fpr, tpr, label='RandomForest (AUC = %0.2f)' % roc_auc)\\n\",\n",
    "    \"#plt.plot([0, 1], [0, 1], 'k--')  # Plot the diagonal line\\n\",\n",
    "    \"plt.xlabel('False Positive Rate')\\n\",\n",
    "    \"plt.ylabel('True Positive Rate')\\n\",\n",
    "    \"#plt.title('Receiver Operating Characteristic (ROC) - Random Forest')\\n\",\n",
    "    \"plt.legend(loc='lower right')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ROC curve for Artificial Neural Networks\\n\",\n",
    "    \"fpr, tpr, thresholds = roc_curve(y_test, ann_pred_2)\\n\",\n",
    "    \"roc_auc = auc(fpr, tpr)\\n\",\n",
    "    \"plt.plot(fpr, tpr, label='ArtificialNeuralNetworks (AUC = %0.2f)' % roc_auc)\\n\",\n",
    "    \"#plt.plot([0, 1], [0, 1], 'k--')  # Plot the diagonal line\\n\",\n",
    "    \"plt.xlabel('False Positive Rate')\\n\",\n",
    "    \"plt.ylabel('True Positive Rate')\\n\",\n",
    "    \"#plt.title('Receiver Operating Characteristic (ROC) - Artificial Neural Network')\\n\",\n",
    "    \"plt.legend(loc='lower right')\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"455c994a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Model Selection<a class=\\\"anchor\\\" id=\\\"modelsel\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"18ba9be5\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"From the above ROC Curves we see that our Logistic Regression model is the best for this dataset, followed by kNN. The AUC of Logistic comes out to be 0.94, while for KNN it is 0.89 and only 0.84 for Decision Tree, 0.88 for Random Forest and 0.83 for Artificial Neural Networks. The Precision and Accuracy levels were also higher for Logisitc Regression, hence we will be moving forward with the Logistic model.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We have also observed that the time taken by the Logistic Regression model takes minimal time to run.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"7a2260f4\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Deployment:<a class=\\\"anchor\\\" id=\\\"deploy\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"* [Go to the Top](#table-of-contents)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 76,\n",
    "   \"id\": \"3b52ad78\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION='python'\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 77,\n",
    "   \"id\": \"1dae0051\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# import pickle\\n\",\n",
    "    \"# pickle_out = open('loanApproval', mode='wb')\\n\",\n",
    "    \"# pickle.dump([lr_model, knn_model, features_means, features_std, X_test.columns], pickle_out)\\n\",\n",
    "    \"# pickle_out.close()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 78,\n",
    "   \"id\": \"4ad937eb\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# %%writefile bankloanapp.py\\n\",\n",
    "    \"\\n\",\n",
    "    \"# import pickle\\n\",\n",
    "    \"# import streamlit as st\\n\",\n",
    "    \"# import math\\n\",\n",
    "    \"# import pandas as pd\\n\",\n",
    "    \"# import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"# pickle_in = open('loanApproval', 'rb')\\n\",\n",
    "    \"# knn_model, lr_model, features_means, features_std, features = pickle.load(pickle_in)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# @st.cache()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# # Define the function which will make the prediction using data\\n\",\n",
    "    \"# # inputs from users\\n\",\n",
    "    \"\\n\",\n",
    "    \"# def prediction(classifier, debt, bank_customer, employment_type, prior_default, employed,\\n\",\n",
    "    \"#                years_employed, credit_score, income):\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     #Populating the booleans\\n\",\n",
    "    \"#     bank_customer = 1 if bank_customer == 'Yes' else 0\\n\",\n",
    "    \"#     prior_default = 0 if prior_default == 'Yes' else 1    #prior_defaulters are identified as 0 in the dataset\\n\",\n",
    "    \"#     employment = 1 if employment == 'Yes' else 0\\n\",\n",
    "    \"#     drivers_license = 1 if drivers_license == 'Yes' else 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     #taking log transform\\n\",\n",
    "    \"#     years_employed = np.log(years_employed)\\n\",\n",
    "    \"#     credit_score = np.log(credit_score)\\n\",\n",
    "    \"#     income = np.log(income)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     #initialization\\n\",\n",
    "    \"#     employment_types = {'Industrial':0,\\n\",\n",
    "    \"#                 'Materials':0,\\n\",\n",
    "    \"#                 'Consumer Services':0,\\n\",\n",
    "    \"#                 'Healthcare':0,\\n\",\n",
    "    \"#                 'Financials':0,\\n\",\n",
    "    \"#                 'Education':0,\\n\",\n",
    "    \"#                 'Utilities':0}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     citizenships = {'Citizen By Birth':0,\\n\",\n",
    "    \"#                     'Temporary Citizenship':0,\\n\",\n",
    "    \"#                     'Other':0}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     #value population\\n\",\n",
    "    \"#     for key in employment_types:\\n\",\n",
    "    \"#         if key == employment_type:\\n\",\n",
    "    \"#             employment_types[key] = 1\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            \\n\",\n",
    "    \"#     X_test_df = pd.DataFrame([[debt, bank_customer, \\n\",\n",
    "    \"#                                employment_types.get('Industrial'),employment_types.get('Materials'),employment_types.get('Consumer Services'),\\n\",\n",
    "    \"#                                employment_types.get('Healthcare'),employment_types.get('Financials'),employment_types.get('Education'),\\n\",\n",
    "    \"#                                employment_types.get('Utilities'),\\n\",\n",
    "    \"#                                prior_default, employment, drivers_license,\\n\",\n",
    "    \"#                                years_employed, credit_score, income]], \\n\",\n",
    "    \"#         columns=features)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     #normalise the test data\\n\",\n",
    "    \"#     X_test_df = (X_test_df - features_means)/features_std\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     # Make predictions\\n\",\n",
    "    \"#     message = ''\\n\",\n",
    "    \"#     if classifier == 'Logistic Regression':\\n\",\n",
    "    \"#         prediction = lr_model.predict_proba(X_test_df)\\n\",\n",
    "    \"#         THRESHOLD = 0.8\\n\",\n",
    "    \"#     else: \\n\",\n",
    "    \"#         prediction = knn_model.predict_proba(X_test_df)\\n\",\n",
    "    \"#         THRESHOLD = 0.6\\n\",\n",
    "    \"#         message = 'AUC for Log Reg is more than AUC for kNN. Accuracy is also better for log reg thus prefer Log Reg'\\n\",\n",
    "    \"\\n\",\n",
    "    \"#     if prediction[0][1] > THRESHOLD:\\n\",\n",
    "    \"#         return ['Your loan is approved :)',message]\\n\",\n",
    "    \"#     else: \\n\",\n",
    "    \"#         return ['Sorry your loan is rejected :(',message]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# # This is the main function in which we define our webpage\\n\",\n",
    "    \"# def main():\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     st.title('Jarvis Loan Approval System')\\n\",\n",
    "    \"#     # Create input fields\\n\",\n",
    "    \"#     classifier = st.radio(\\\"Which model you want to use for prediction?\\\",\\n\",\n",
    "    \"#                             ('kNN','Logistic Regression'))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     bank_customer = st.radio('Is the borrower already bank customer',\\n\",\n",
    "    \"#                              ('Yes','No'))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     prior_default = st.radio('Is the borrower prior defaulter?',\\n\",\n",
    "    \"#                              ('Yes','No'))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     employment = st.radio('Is the borrower employed?',\\n\",\n",
    "    \"#                              ('Yes','No'))\\n\",\n",
    "    \"       \\n\",\n",
    "    \"#    drivers_license = st.radio('Does the borrower have drivers license?',\\n\",\n",
    "    \"#                             ('Yes','No'))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     debt = st.number_input(\\\"How much debt does borrower already have in $1000s?\\\",\\n\",\n",
    "    \"#                                   min_value=0.000,\\n\",\n",
    "    \"#                                   max_value=30.000,\\n\",\n",
    "    \"#                                   value=10.000,\\n\",\n",
    "    \"#                                   step=2.500,\\n\",\n",
    "    \"#                                  )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     employment_type = st.selectbox('Select the employment type',\\n\",\n",
    "    \"#                              ('Industrial','Materials','Consumer Services','Healthcare','Financials','Utilities','Education'))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"               \\n\",\n",
    "    \"#     years_employed = st.number_input(\\\"For how many years the borrower has been employed?\\\",\\n\",\n",
    "    \"#                                   min_value=1.0,\\n\",\n",
    "    \"#                                   max_value=30.0,\\n\",\n",
    "    \"#                                   value=10.0,\\n\",\n",
    "    \"#                                   step=1.0,\\n\",\n",
    "    \"#                                  )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     credit_score = st.number_input(\\\"What is the credit score of borrower?\\\",\\n\",\n",
    "    \"#                                   min_value=1.0,\\n\",\n",
    "    \"#                                   max_value=70.0,\\n\",\n",
    "    \"#                                   value=10.0,\\n\",\n",
    "    \"#                                   step=5.0,\\n\",\n",
    "    \"#                                  )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     income = st.number_input(\\\"What is the income of borrower?\\\",\\n\",\n",
    "    \"#                                   min_value=1,\\n\",\n",
    "    \"#                                   max_value=50000,\\n\",\n",
    "    \"#                                   value=30000,\\n\",\n",
    "    \"#                                   step=5000,\\n\",\n",
    "    \"#                                  )\\n\",\n",
    "    \"               \\n\",\n",
    "    \"#     result = \\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#     # When 'Predict' is clicked, make the prediction and store it\\n\",\n",
    "    \"#     if st.button(\\\"Should be approve the loan?\\\"):\\n\",\n",
    "    \"#         result = prediction(classifier, debt, bank_customer, employment_type, prior_default, employment, drivers_license,\\n\",\n",
    "    \"#                years_employed, credit_score, income)\\n\",\n",
    "    \"#         st.success(result[0])\\n\",\n",
    "    \"#         if classifier == 'kNN':\\n\",\n",
    "    \"#             st.success(result[1])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"# if __name__=='__main__':\\n\",\n",
    "    \"#     main()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 79,\n",
    "   \"id\": \"9e41cbb4\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# !streamlit run bankloanapp.py\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"052dedbc\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
